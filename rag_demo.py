import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'  # è®¾ç½® Hugging Face é•œåƒæºï¼Œè§£å†³å›½å†…æ— æ³•ç›´æ¥ä¸‹è½½æ¨¡å‹çš„é—®é¢˜
import gradio as gr  # æ„å»º Web UI çš„åº“
from pdfminer.high_level import extract_text_to_fp  # ä» PDF ä¸­æå–æ–‡æœ¬
from sentence_transformers import SentenceTransformer  # æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹
from sentence_transformers import CrossEncoder  # å¯¼å…¥äº¤å‰ç¼–ç å™¨ï¼Œç”¨äºç»†ç²’åº¦è¯­ä¹‰åŒ¹é…ï¼ˆå¦‚é‡æ’åºé˜¶æ®µï¼‰
from faiss import IndexFlatL2, IndexIVFFlat, IndexIVFPQ # Facebook AI çš„å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢åº“ï¼ˆç”¨äºæ„å»ºå‘é‡ç´¢å¼•ï¼‰
import requests  # ç½‘ç»œè¯·æ±‚åº“ï¼Œç”¨äº API è¯·æ±‚ã€æ¨¡å‹ä¸‹è½½ç­‰
import json  # å¤„ç† JSON æ ¼å¼æ•°æ®
from io import StringIO  # å°† PDF æå–å†…å®¹å†™å…¥å†…å­˜å¯¹è±¡ä¸­
from langchain_text_splitters import RecursiveCharacterTextSplitter  # æ–‡æœ¬åˆ†æ®µå·¥å…·
import os  # å†æ¬¡å¯¼å…¥ osï¼ˆå¯çœç•¥ï¼Œå±äºå†—ä½™ï¼‰
import socket  # åˆ¤æ–­ç«¯å£å ç”¨æˆ–æœ¬åœ°ç½‘ç»œæ£€æŸ¥
import webbrowser  # è‡ªåŠ¨æ‰“å¼€ Web UI é¡µé¢
import logging  # æ—¥å¿—è®°å½•ï¼Œç”¨äºè°ƒè¯•ä¸é”™è¯¯è·Ÿè¸ª
from requests.adapters import HTTPAdapter  # è¯·æ±‚é‡è¯•æœºåˆ¶
from urllib3.util.retry import Retry
import time  # æ—¶é—´å¤„ç†ï¼Œç”¨äºè®¡æ—¶æˆ–å»¶è¿Ÿ
from datetime import datetime  # è·å–æ—¶é—´æˆ³ã€æ—¥å¿—æ—¶é—´ç­‰
import re  # æ­£åˆ™å¤„ç†ï¼Œé€‚ç”¨äº PDF å†…å®¹æ¸…æ´—ç­‰
from dotenv import load_dotenv  # åŠ è½½ç¯å¢ƒå˜é‡é…ç½®ï¼ˆå¦‚ API KEYï¼‰
from rank_bm25 import BM25Okapi  # å¯¼å…¥ BM25 ç®—æ³•ï¼Œç”¨äºä¼ ç»Ÿç¨€ç–æ£€ç´¢ï¼Œä¸å‘é‡æœç´¢å¯æ··åˆå¢å¼ºå¬å›ç‡
import numpy as np  # å‘é‡è®¡ç®—ã€FAISS ä¾èµ–çš„æ•°å€¼åº“
import jieba  # ä¸­æ–‡åˆ†è¯åº“ï¼Œç”¨äº BM25 ä¸­æ–‡æ£€ç´¢æ•ˆæœæå‡
import threading  # å¤šçº¿ç¨‹åŠ é€Ÿå¤„ç†ï¼Œä¾‹å¦‚æ–‡æ¡£å‘é‡åŒ–
from pathlib import Path  # é«˜çº§æ–‡ä»¶è·¯å¾„å¤„ç†
from functools import lru_cache  # ç¼“å­˜è£…é¥°å™¨ï¼Œæé«˜é‡å¤è°ƒç”¨æ€§èƒ½
from typing import List, Tuple, Any, Optional  # ç±»å‹æ³¨è§£ï¼Œæé«˜ä»£ç å¯è¯»æ€§ä¸ç»´æŠ¤æ€§


# åŠ è½½ç¯å¢ƒå˜é‡
# åŠ è½½ .env é…ç½®æ–‡ä»¶è·¯å¾„
dotenv_path = Path(__file__).parent / "example.env"
load_dotenv(dotenv_path)
# ä» .env æ–‡ä»¶ä¸­è¯»å– SERPAPI_KEY ç”¨äºæœç´¢å¼•æ“æŸ¥è¯¢
SERPAPI_KEY = os.getenv("SERPAPI_KEY")  # åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® SERPAPI_KEY
# é»˜è®¤æœç´¢å¼•æ“ä¸º Googleï¼Œå¦‚éœ€å¯åˆ‡æ¢è‡³å…¶ä»–æœç´¢æº
SEARCH_ENGINE = "google"
RERANK_METHOD = os.getenv("RERANK_METHOD", "cross_encoder")

#  SiliconFlow API é…ç½®ï¼Œç”¨äº LLM è°ƒç”¨
SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")  # åœ¨ .env è®¾ç½® API Key
SILICONFLOW_API_URL = os.getenv(
    "SILICONFLOW_API_URL",
    "https://api.siliconflow.cn/v1/chat/completions"
)  # é»˜è®¤è®¿é—® SiliconFlow Chat API

# æ¨¡å‹åç§°é…ç½®ï¼Œæ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è‡ªå®šä¹‰æœ¬åœ°å’Œäº‘ç«¯æ¨¡å‹
OLLAMA_MODEL_NAME = os.getenv("OLLAMA_MODEL_NAME", "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B")
SILICONFLOW_MODEL_NAME = os.getenv("SILICONFLOW_MODEL_NAME", "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B")

# è®¾ç½®è¯·æ±‚è¶…æ—¶ä¸é‡è¯•æœºåˆ¶ï¼Œæå‡ç½‘ç»œè®¿é—®çš„ç¨³å®šæ€§
requests.adapters.DEFAULT_RETRIES = 3  # å¢åŠ ç½‘ç»œè¯·æ±‚å¤±è´¥åçš„é‡è¯•æ¬¡æ•°

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå…³é—­ TensorFlow çš„ oneDNN ä¼˜åŒ–ï¼Œé¿å…æŸäº› CPU ç¯å¢ƒä¸‹çš„å…¼å®¹æ€§é—®é¢˜
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

# é…ç½®ä»£ç†ç»•è¿‡è§„åˆ™ï¼Œé¿å…æœ¬åœ°è®¿é—®ï¼ˆå¦‚ 127.0.0.1ï¼‰èµ°ä»£ç†å¯¼è‡´æ— æ³•è¿æ¥
os.environ['NO_PROXY'] = 'localhost,127.0.0.1'

# åˆå§‹åŒ–å‘é‡åµŒå…¥æ¨¡å‹ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡(è‹±æ–‡ä¼˜åŒ–æ¨¡å‹)
EMBED_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
# è‹¥ä¸»è¦å¤„ç†ä¸­æ–‡æ–‡æ¡£ï¼Œå¯åˆ‡æ¢ä¸ºä¸­æ–‡ä¼˜åŒ–æ¨¡å‹ï¼Œä¾‹å¦‚ï¼š
# EMBED_MODEL = SentenceTransformer('shibing624/text2vec-base-chinese')

# FAISS ç›¸å…³çš„å…¨å±€æ•°æ®ç»“æ„
faiss_index = None  # FAISS ç´¢å¼•å¯¹è±¡ï¼Œå­˜å‚¨å‘é‡ç”¨äºç›¸ä¼¼åº¦æ£€ç´¢
faiss_contents_map = {}  # original_id -> content æ˜ å°„ï¼Œç”¨äºæ£€ç´¢ç»“æœçš„åŸæ–‡è¿˜åŸ
faiss_metadatas_map = {}  # original_id -> metadata æ˜ å°„ï¼Œç”¨äºè®°å½•æ–‡æ¡£ç‰‡æ®µå…ƒä¿¡æ¯
faiss_id_order_for_index = []  # è®°å½•å‘é‡æ·»åŠ åˆ° FAISS çš„é¡ºåºï¼Œä¾¿äºç®¡ç†å’Œå¢é‡æ›´æ–°

# åˆå§‹åŒ–äº¤å‰ç¼–ç å™¨ï¼Œç”¨äºé‡æ’åºç»“æœã€‚é‡‡å–æ‡’åŠ è½½ç­–ç•¥ï¼Œé¦–æ¬¡ä½¿ç”¨æ—¶å†åŠ è½½æ¨¡å‹
cross_encoder = None
cross_encoder_lock = threading.Lock()  # çº¿ç¨‹é”ï¼Œé¿å…å¤šçº¿ç¨‹ç¯å¢ƒä¸­é‡å¤åŠ è½½äº¤å‰ç¼–ç å™¨

def get_cross_encoder():
    """æ‡’åŠ è½½äº¤å‰ç¼–ç å™¨æ¨¡å‹ï¼Œé¿å…é¦–æ¬¡è¿è¡Œé˜»å¡"""
    global cross_encoder
    if cross_encoder is None:
        with cross_encoder_lock:
            if cross_encoder is None:
                try:
                    # å¤šè¯­è¨€äº¤å‰ç¼–ç å™¨ï¼Œæ›´é€‚åˆä¸­æ–‡è¯­ä¹‰é‡æ’åº,ä¸“é—¨ç”¨æ¥å¯¹ã€Œåˆæ­¥æ£€ç´¢å›æ¥çš„å€™é€‰ç»“æœã€åšæ›´ç²¾ç»†çš„è¯­ä¹‰ç›¸å…³æ€§æ‰“åˆ†ï¼Œä»è€Œæå‡æœ€ç»ˆæ’åºè´¨é‡
                    cross_encoder = CrossEncoder(
                        'sentence-transformers/distiluse-base-multilingual-cased-v2'
                    )
                    logging.info("äº¤å‰ç¼–ç å™¨åŠ è½½æˆåŠŸ")
                except Exception as e:
                    logging.error(f"åŠ è½½äº¤å‰ç¼–ç å™¨å¤±è´¥: {str(e)}")
                    # ä¿æŒ Noneï¼Œä»¥ä¾¿åç»­é‡è¯•åŠ è½½
                    cross_encoder = None
    return cross_encoder

# æ–°å¢ï¼šè‡ªåŠ¨é€‰æ‹©FAISSç´¢å¼•ç±»å‹çš„å°è£…ç±»
class AutoFaissIndex:
    def __init__(self, dimension=384):
        """
        è‡ªåŠ¨é€‰æ‹©FAISSç´¢å¼•ç±»å‹çš„å°è£…ç±»
        å‚æ•°:
            dimension: å‘é‡ç»´åº¦ (é»˜è®¤ä¸º384ï¼Œå¯¹åº”all-MiniLM-L6-v2æ¨¡å‹çš„è¾“å‡ºç»´åº¦)
        """
        self.dimension = dimension
        self.index = None
        self.index_type = None
        self.nlist = None  # IVFç±»ç´¢å¼•çš„èšç±»ä¸­å¿ƒæ•°
        self.m = None  # PQç±»ç´¢å¼•çš„ç»†åˆ†æ®µæ•°
        self.nprobe = None  # IVFç±»ç´¢å¼•çš„æœç´¢èšç±»ä¸­å¿ƒæ•°

        # é˜ˆå€¼é…ç½® (å¯æ ¹æ®å®é™…ç¡¬ä»¶è°ƒæ•´)
        self.small_dataset_threshold = 10_000  # å°æ•°æ®é›†é˜ˆå€¼
        self.medium_dataset_threshold = 100_000  # ä¸­ç­‰æ•°æ®é›†é˜ˆå€¼
        self.large_dataset_threshold = 1_000_000  # å¤§æ•°æ®é›†é˜ˆå€¼

    @property
    def ntotal(self):
        """è¿”å›ç´¢å¼•ä¸­çš„å‘é‡æ€»æ•°"""
        return self.index.ntotal if self.index else 0

    def select_index_type(self, num_vectors):
        """
        æ ¹æ®å‘é‡æ•°é‡è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç´¢å¼•ç±»å‹
        å‚æ•°:
            num_vectors: è¦ç´¢å¼•çš„å‘é‡æ•°é‡
        """
        if num_vectors <= self.small_dataset_threshold:
            # å°æ•°æ®é›†: ä½¿ç”¨ç²¾ç¡®æœç´¢çš„Flatç´¢å¼•
            self.index_type = "FlatL2"
            self.index = IndexFlatL2(self.dimension)
            self.nprobe = 1  # ä¸ç›¸å…³ï¼Œä»…ä¸ºç»Ÿä¸€æ¥å£

        elif num_vectors <= self.medium_dataset_threshold:
            # ä¸­ç­‰æ•°æ®é›†: ä½¿ç”¨IVFFlatå¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦
            self.index_type = "IVFFlat"
            self.nlist = min(100, int(np.sqrt(num_vectors)))
            quantizer = IndexFlatL2(self.dimension)
            self.index = IndexIVFFlat(quantizer, self.dimension, self.nlist)
            self.nprobe = min(10, max(1, int(self.nlist * 0.1)))  # æœç´¢10%çš„èšç±»ä¸­å¿ƒ

        else:
            # å¤§æ•°æ®é›†: ä½¿ç”¨IVFPQç‰ºç‰²å°‘é‡ç²¾åº¦æ¢å–æ›´é«˜æ•ˆç‡
            self.index_type = "IVFPQ"
            self.nlist = min(256, int(np.sqrt(num_vectors)))
            self.m = min(8, self.dimension // 4)  # æ¯ä¸ªå‘é‡åˆ†æˆ8æ®µ
            quantizer = IndexFlatL2(self.dimension)
            self.index = IndexIVFPQ(quantizer, self.dimension, self.nlist, self.m, 8)  # 8 bits per code
            self.nprobe = min(32, max(1, int(self.nlist * 0.05)))  # æœç´¢5%çš„èšç±»ä¸­å¿ƒ

        return self.index_type

    def train(self, vectors):
        """
        è®­ç»ƒç´¢å¼• (ä»…IVFç±»ç´¢å¼•éœ€è¦)

        å‚æ•°:
            vectors: ç”¨äºè®­ç»ƒçš„å‘é‡æ•°ç»„ (np.array)
        """
        if self.index_type in ["IVFFlat", "IVFPQ"]:
            self.index.train(vectors)

    def add(self, vectors):
        """
        æ·»åŠ å‘é‡åˆ°ç´¢å¼•

        å‚æ•°:
            vectors: è¦æ·»åŠ çš„å‘é‡æ•°ç»„ (np.array)
        """
        if self.index_type in ["IVFFlat", "IVFPQ"] and not self.index.is_trained:
            self.train(vectors)

        self.index.add(vectors)

    def search(self, query_vectors, k=5):
        """
        æ‰§è¡Œæœç´¢

        å‚æ•°:
            query_vectors: æŸ¥è¯¢å‘é‡æ•°ç»„ (np.array)
            k: è¿”å›çš„æœ€è¿‘é‚»æ•°é‡

        è¿”å›:
            distances: è·ç¦»çŸ©é˜µ (nq, k)
            indices: ç´¢å¼•çŸ©é˜µ (nq, k)
        """
        if self.index_type in ["IVFFlat", "IVFPQ"]:
            self.index.nprobe = self.nprobe

        return self.index.search(query_vectors, k)

    def get_index_info(self):
        """è·å–å½“å‰ç´¢å¼•é…ç½®ä¿¡æ¯"""
        return {
            "index_type": self.index_type,
            "dimension": self.dimension,
            "nlist": self.nlist,
            "m": self.m,
            "nprobe": self.nprobe,
            "size": self.ntotal
        }

# é€’å½’æ£€ç´¢ä¸»é€»è¾‘
# å¤šè½®æ£€ç´¢ â†’ æ··åˆæ’åº â†’ äº¤å‰ç¼–ç å™¨ â†’ LLM å¼•å¯¼ Query æ‰©å±•
def recursive_retrieval(initial_query, max_iterations=3, enable_web_search=False, model_choice="siliconflow"):
    """
    é€’å½’æ£€ç´¢ä¸æŸ¥è¯¢è¿­ä»£ä¼˜åŒ–
    åˆ©ç”¨ç°æœ‰æ£€ç´¢ç»“æœï¼Œé€šè¿‡ LLM åˆ¤æ–­æ˜¯å¦ç»§ç»­æ”¹å†™æŸ¥è¯¢è¿›è¡Œä¸‹ä¸€è½®æ£€ç´¢

    Args:
        initial_query: åˆå§‹æŸ¥è¯¢æ–‡æœ¬
        max_iterations: æœ€å¤§æ£€ç´¢è¿­ä»£è½®æ•°
        enable_web_search: æ˜¯å¦å¯ç”¨äº’è”ç½‘è¡¥å……æœç´¢ç»“æœ
        model_choice: ä½¿ç”¨æ¨ç†æ¨¡å‹æ¥æº("ollama" æˆ– "siliconflow(ç¡…åŸºæµåŠ¨)")

    Returns:
        all_contexts: æ‰€æœ‰è·å–åˆ°çš„æ–‡æœ¬å†…å®¹åˆ—è¡¨
        all_doc_ids: æ–‡æœ¬å¯¹åº”çš„åŸå§‹ ID åˆ—è¡¨
        all_metadata: æ–‡æœ¬å…ƒä¿¡æ¯åˆ—è¡¨
    """
    query = initial_query
    all_contexts = []
    all_doc_ids = []
    all_metadata = []

    global faiss_index, faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index

    for i in range(max_iterations):
        logging.info(f"é€’å½’æ£€ç´¢ {i + 1}/{max_iterations}ï¼Œå½“å‰ Query: {query}")

        # å­˜å‚¨æ¥è‡ªç½‘ç»œæœç´¢çš„è¡¥å……ä¿¡æ¯ï¼Œä¼˜å…ˆæä¾›ç»™ LLM åˆ¤æ–­
        web_results_texts = []

        if enable_web_search and check_serpapi_key():
            try:
                web_search_raw_results = update_web_results(query)
                for res in web_search_raw_results:
                    # ä»…åŠ å…¥å†…å®¹ï¼Œä¸åŠ å…¥å‘é‡åº“ï¼Œé¿å…åœ¨çº¿æ¥æºæ±¡æŸ“æœ¬åœ°ç´¢å¼•
                    text = f"æ ‡é¢˜ï¼š{res.get('title', '')}\næ‘˜è¦ï¼š{res.get('snippet', '')}"
                    web_results_texts.append(text)
            except Exception as e:
                logging.error(f"ç½‘ç»œæœç´¢å‡ºé”™: {str(e)}")

        # è¯­ä¹‰æ£€ç´¢
        query_embedding = EMBED_MODEL.encode([query])
        query_embedding_np = np.array(query_embedding).astype('float32')

        semantic_results_docs = []
        semantic_results_metadatas = []
        semantic_results_ids = []

        # ä¿®å¤FAISSæ£€ç´¢éƒ¨åˆ†
        if faiss_index is not None and hasattr(faiss_index, 'ntotal') and faiss_index.ntotal > 0:
            try:
                D, I = faiss_index.search(query_embedding_np, k=10)
                # å°†FAISSç´¢å¼•è½¬æ¢å›åŸå§‹ID
                for faiss_idx in I[0]:
                    if faiss_idx != -1 and faiss_idx < len(faiss_id_order_for_index):
                        original_id = faiss_id_order_for_index[faiss_idx]
                        if original_id in faiss_contents_map:  # æ·»åŠ å­˜åœ¨æ€§æ£€æŸ¥
                            semantic_results_docs.append(faiss_contents_map.get(original_id, ""))
                            semantic_results_metadatas.append(faiss_metadatas_map.get(original_id, {}))
                            semantic_results_ids.append(original_id)
                        else:
                            logging.warning(f"ID {original_id} ä¸åœ¨å†…å®¹æ˜ å°„ä¸­")
            except Exception as e:
                logging.error(f"FAISS æ£€ç´¢é”™è¯¯: {str(e)}")
        else:
            logging.warning("FAISSç´¢å¼•ä¸ºç©ºæˆ–æœªåˆå§‹åŒ–")

        # ç¨€ç–æ£€ç´¢ BM25
        bm25_results = BM25_MANAGER.search(query, top_k=10) if BM25_MANAGER.bm25_index else []

        # æ ¼å¼å¯¹é½ï¼Œé€‚é…æ··åˆæ’åºå‡½æ•°
        prepared_semantic_results_for_hybrid = {
            "ids": [semantic_results_ids],
            "documents": [semantic_results_docs],
            "metadatas": [semantic_results_metadatas]
        }

        # æ··åˆè¯­ä¹‰ + BM25 æ’åº
        hybrid_results = hybrid_merge(prepared_semantic_results_for_hybrid, bm25_results, alpha=0.7)

        doc_ids_current_iter = []
        docs_current_iter = []
        metadata_list_current_iter = []

        # å– Top-10 è¿›å…¥äº¤å‰ç¼–ç é‡æ’åº
        if hybrid_results:
            for doc_id, result_data in hybrid_results[:10]:
                doc_ids_current_iter.append(doc_id)
                docs_current_iter.append(result_data['content'])
                metadata_list_current_iter.append(result_data['metadata'])

        # äº¤å‰ç¼–ç å™¨é‡æ’åºï¼Œæå‡å‡†ç¡®æ€§
        if docs_current_iter:
            try:
                reranked_results = rerank_results(query, docs_current_iter, doc_ids_current_iter,
                                                  metadata_list_current_iter, top_k=5)
            except Exception as e:
                logging.error(f"é‡æ’åºå¤±è´¥: {str(e)}")
                # å›é€€ä¸ºæ··åˆæ’åºç»“æœ
                reranked_results = [
                    (doc_id, {'content': doc, 'metadata': meta, 'score': 1.0})
                    for doc_id, doc, meta in zip(doc_ids_current_iter, docs_current_iter, metadata_list_current_iter)
                ]
        else:
            reranked_results = []

        # æ•´åˆæœ¬è½®æ£€ç´¢ç»“æœ
        current_contexts_for_llm = web_results_texts[:]
        for doc_id, result_data in reranked_results:
            if doc_id not in all_doc_ids:
                all_doc_ids.append(doc_id)
                all_contexts.append(result_data['content'])
                all_metadata.append(result_data['metadata'])
            current_contexts_for_llm.append(result_data['content'])

        if i == max_iterations - 1:
            break

        # è°ƒç”¨ LLM å†³ç­–æ˜¯å¦ç”Ÿæˆæ–°çš„æŸ¥è¯¢
        if current_contexts_for_llm:
            current_summary = "\n".join(current_contexts_for_llm[:3])

            next_query_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢ä¼˜åŒ–åŠ©æ‰‹ã€‚æ ¹æ®ä»¥ä¸‹ä¿¡æ¯åˆ¤æ–­æ˜¯å¦éœ€è¦æ–°çš„æŸ¥è¯¢ã€‚

[åˆå§‹é—®é¢˜]
{initial_query}

[æ£€ç´¢ç»“æœæ‘˜è¦]
{current_summary}

è¦æ±‚ï¼š
1. å¦‚æœä¿¡æ¯å·²è¶³å¤Ÿï¼Œç›´æ¥å›å¤ï¼šä¸éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢
2. å¦åˆ™è¿”å›ä¸€ä¸ªæ›´ç²¾å‡†çš„æ–°æŸ¥è¯¢ï¼Œä»…åŒ…å«æŸ¥è¯¢è¯
"""

            try:
                if model_choice == "siliconflow":
                    logging.info("ä½¿ç”¨ SiliconFlow API åˆ†æä¸‹ä¸€æ­¥æŸ¥è¯¢")
                    result = call_siliconflow_api(next_query_prompt)
                    next_query = result.strip() if isinstance(result, str) else result[0].strip()

                    if "<think>" in next_query:
                        next_query = next_query.split("<think>")[0].strip()

                else:
                    logging.info("ä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹åˆ†æä¸‹ä¸€æ­¥æŸ¥è¯¢")
                    response = session.post(
                        "http://localhost:11434/api/generate",
                        json={
                            "model": OLLAMA_MODEL_NAME,
                            "prompt": next_query_prompt,
                            "stream": False
                        },
                        timeout=180
                    )
                    next_query = response.json().get("response", "").strip()

                if "ä¸éœ€è¦" in next_query:
                    logging.info("LLM åˆ¤æ–­æ— éœ€æ›´å¤šæŸ¥è¯¢")
                    break

                if len(next_query) > 100:
                    logging.warning("ç”Ÿæˆå†…å®¹è¿‡é•¿ï¼Œä¸è§†ä¸ºæœ‰æ•ˆæŸ¥è¯¢")
                    break

                query = next_query
                logging.info(f"ç”Ÿæˆä¸‹ä¸€è½®æŸ¥è¯¢: {query}")

            except Exception as e:
                logging.error(f"ç”Ÿæˆæ–°æŸ¥è¯¢å¤±è´¥: {str(e)}")
                break
        else:
            break

    return all_contexts, all_doc_ids, all_metadata


class BM25IndexManager:
    def __init__(self):
        self.bm25_index = None
        self.doc_mapping = {}  # æ˜ å°„BM25ç´¢å¼•ä½ç½®åˆ°æ–‡æ¡£ID
        self.tokenized_corpus = []
        self.raw_corpus = []

    def build_index(self, documents, doc_ids):
        """æ„å»ºBM25ç´¢å¼•"""
        self.raw_corpus = documents
        self.doc_mapping = {i: doc_id for i, doc_id in enumerate(doc_ids)}

        # å¯¹æ–‡æ¡£è¿›è¡Œåˆ†è¯ï¼Œä½¿ç”¨jiebaåˆ†è¯å™¨æ›´é€‚åˆä¸­æ–‡
        self.tokenized_corpus = []
        for doc in documents:
            # å¯¹ä¸­æ–‡æ–‡æ¡£è¿›è¡Œåˆ†è¯
            tokens = list(jieba.cut(doc))
            self.tokenized_corpus.append(tokens)

        # åˆ›å»ºBM25ç´¢å¼•
        self.bm25_index = BM25Okapi(self.tokenized_corpus)
        return True

    def search(self, query, top_k=5):
        """ä½¿ç”¨BM25æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if not self.bm25_index:
            return []

        # å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
        tokenized_query = list(jieba.cut(query))

        # è·å–BM25å¾—åˆ†
        bm25_scores = self.bm25_index.get_scores(tokenized_query)

        # è·å–å¾—åˆ†æœ€é«˜çš„æ–‡æ¡£ç´¢å¼•
        top_indices = np.argsort(bm25_scores)[-top_k:][::-1]

        # è¿”å›ç»“æœ
        results = []
        for idx in top_indices:
            if bm25_scores[idx] > 0:  # åªè¿”å›æœ‰ç›¸å…³æ€§çš„ç»“æœ
                results.append({
                    'id': self.doc_mapping[idx],
                    'score': float(bm25_scores[idx]),
                    'content': self.raw_corpus[idx]
                })

        return results

    def clear(self):
        """æ¸…ç©ºç´¢å¼•"""
        self.bm25_index = None
        self.doc_mapping = {}
        self.tokenized_corpus = []
        self.raw_corpus = []


# åˆå§‹åŒ–BM25ç´¢å¼•ç®¡ç†å™¨
BM25_MANAGER = BM25IndexManager()

logging.basicConfig(level=logging.INFO)

print("Gradio version:", gr.__version__)  # æ·»åŠ ç‰ˆæœ¬è¾“å‡º

# åœ¨åˆå§‹åŒ–ç»„ä»¶åæ·»åŠ ï¼š
session = requests.Session()
retries = Retry(
    total=3,
    backoff_factor=0.1,
    status_forcelist=[500, 502, 503, 504]
)
session.mount('http://', HTTPAdapter(max_retries=retries))


#########################################
# SerpAPI ç½‘ç»œæŸ¥è¯¢åŠå‘é‡åŒ–å¤„ç†å‡½æ•°
#########################################
def serpapi_search(query: str, num_results: int = 5) -> list:
    """
    æ‰§è¡Œ SerpAPI æœç´¢ï¼Œå¹¶è¿”å›è§£æåçš„ç»“æ„åŒ–ç»“æœ
    """
    if not SERPAPI_KEY:
        raise ValueError("æœªè®¾ç½® SERPAPI_KEY ç¯å¢ƒå˜é‡ã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„ API å¯†é’¥ã€‚")
    try:
        params = {
            "engine": SEARCH_ENGINE,
            "q": query,
            "api_key": SERPAPI_KEY,
            "num": num_results,
            "hl": "zh-CN",  # ä¸­æ–‡ç•Œé¢
            "gl": "cn"
        }
        response = requests.get("https://serpapi.com/search", params=params, timeout=15)
        response.raise_for_status()
        search_data = response.json()
        return _parse_serpapi_results(search_data)
    except Exception as e:
        logging.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
        return []


def _parse_serpapi_results(data: dict) -> list:
    """è§£æ SerpAPI è¿”å›çš„åŸå§‹æ•°æ®"""
    results = []
    if "organic_results" in data:
        for item in data["organic_results"]:
            result = {
                "title": item.get("title"),
                "url": item.get("link"),
                "snippet": item.get("snippet"),
                "timestamp": item.get("date")  # è‹¥æœ‰æ—¶é—´ä¿¡æ¯ï¼Œå¯é€‰
            }
            results.append(result)
    # å¦‚æœæœ‰çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œä¹Ÿå¯ä»¥æ·»åŠ ç½®é¡¶ï¼ˆå¯é€‰ï¼‰
    if "knowledge_graph" in data:
        kg = data["knowledge_graph"]
        results.insert(0, {
            "title": kg.get("title"),
            "url": kg.get("source", {}).get("link", ""),
            "snippet": kg.get("description"),
            "source": "knowledge_graph"
        })
    return results


def update_web_results(query: str, num_results: int = 5) -> list:
    """
    åŸºäº SerpAPI æœç´¢ç»“æœã€‚æ³¨æ„ï¼šæ­¤ç‰ˆæœ¬ä¸å°†ç»“æœå­˜å…¥FAISSã€‚
    å®ƒä»…è¿”å›åŸå§‹æœç´¢ç»“æœã€‚
    """
    results = serpapi_search(query, num_results)
    if not results:
        logging.info("ç½‘ç»œæœç´¢æ²¡æœ‰è¿”å›ç»“æœæˆ–å‘ç”Ÿé”™è¯¯")
        return []

    # ä¹‹å‰è¿™é‡Œæœ‰åˆ é™¤æ—§ç½‘ç»œç»“æœå’Œæ·»åŠ åˆ°ChromaDBçš„é€»è¾‘ã€‚
    # ç”±äºFAISS IndexFlatL2ä¸æ”¯æŒæŒ‰IDåˆ é™¤ï¼Œå¹¶ä¸”åŠ¨æ€æ·»åŠ æ¶‰åŠå¤æ‚IDç®¡ç†ï¼Œ
    # æ­¤ç®€åŒ–ç‰ˆæœ¬ä¸å°†ç½‘ç»œç»“æœæ·»åŠ åˆ°FAISSç´¢å¼•ã€‚
    # è¿”å›åŸå§‹ç»“æœï¼Œä¾›è°ƒç”¨è€…å†³å®šå¦‚ä½•ä½¿ç”¨ï¼ˆä¾‹å¦‚ï¼Œä»…ä½œä¸ºæ–‡æœ¬ä¸Šä¸‹æ–‡ï¼‰ã€‚
    logging.info(f"ç½‘ç»œæœç´¢è¿”å› {len(results)} æ¡ç»“æœï¼Œè¿™äº›ç»“æœä¸ä¼šè¢«æ·»åŠ åˆ°FAISSç´¢å¼•ä¸­ã€‚")
    return results  # è¿”å›åŸå§‹SerpAPIç»“æœåˆ—è¡¨


# æ£€æŸ¥æ˜¯å¦é…ç½®äº†SERPAPI_KEY
def check_serpapi_key():
    """æ£€æŸ¥æ˜¯å¦é…ç½®äº†SERPAPI_KEY"""
    return SERPAPI_KEY is not None and SERPAPI_KEY.strip() != ""


# æ·»åŠ æ–‡ä»¶å¤„ç†çŠ¶æ€è·Ÿè¸ª
class FileProcessor:
    def __init__(self):
        self.processed_files = {}  # å­˜å‚¨å·²å¤„ç†æ–‡ä»¶çš„çŠ¶æ€

    def clear_files(self):
        """æ¸…ç©ºæ‰€æœ‰æ–‡ä»¶è®°å½•"""
        self.processed_files = {}

    def add_file(self, file_name):
        self.processed_files[file_name] = {
            'status': 'ç­‰å¾…å¤„ç†',
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'chunks': 0
        }

    def update_status(self, file_name, status, chunks=None):
        if file_name in self.processed_files:
            self.processed_files[file_name]['status'] = status
            if chunks is not None:
                self.processed_files[file_name]['chunks'] = chunks

    def get_file_list(self):
        return [
            f"ğŸ“„ {fname} | {info['status']}"
            for fname, info in self.processed_files.items()
        ]


file_processor = FileProcessor()


#########################################
# çŸ›ç›¾æ£€æµ‹å‡½æ•°
#########################################
def detect_conflicts(sources):
    """ç²¾å‡†çŸ›ç›¾æ£€æµ‹ç®—æ³•"""
    key_facts = {}
    for item in sources:
        facts = extract_facts(item['text'] if 'text' in item else item.get('excerpt', ''))
        for fact, value in facts.items():
            if fact in key_facts:
                if key_facts[fact] != value:
                    return True
            else:
                key_facts[fact] = value
    return False


def extract_facts(text):
    """ä»æ–‡æœ¬æå–å…³é”®äº‹å®ï¼ˆç¤ºä¾‹é€»è¾‘ï¼‰"""
    facts = {}
    # æå–æ•°å€¼å‹äº‹å®
    numbers = re.findall(r'\b\d{4}å¹´|\b\d+%', text)
    if numbers:
        facts['å…³é”®æ•°å€¼'] = numbers
    # æå–æŠ€æœ¯æœ¯è¯­
    if "äº§ä¸šå›¾è°±" in text:
        facts['æŠ€æœ¯æ–¹æ³•'] = list(set(re.findall(r'[A-Za-z]+æ¨¡å‹|[A-Z]{2,}ç®—æ³•', text)))
    return facts


def evaluate_source_credibility(source):
    """è¯„ä¼°æ¥æºå¯ä¿¡åº¦"""
    credibility_scores = {
        "gov.cn": 0.9,
        "edu.cn": 0.85,
        "weixin": 0.7,
        "zhihu": 0.6,
        "baidu": 0.5
    }

    url = source.get('url', '')
    if not url:
        return 0.5  # é»˜è®¤ä¸­ç­‰å¯ä¿¡åº¦

    domain_match = re.search(r'//([^/]+)', url)
    if not domain_match:
        return 0.5

    domain = domain_match.group(1)

    # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•å·²çŸ¥åŸŸå
    for known_domain, score in credibility_scores.items():
        if known_domain in domain:
            return score

    return 0.5  # é»˜è®¤ä¸­ç­‰å¯ä¿¡åº¦


# ä¿®æ”¹åçš„ extract_text å‡½æ•°, æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼
def extract_text(filepath):
    """æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼çš„æ–‡æœ¬æå–"""
    file_ext = os.path.splitext(filepath)[1].lower()

    if file_ext == '.pdf':
        output = StringIO()
        with open(filepath, 'rb') as file:
            extract_text_to_fp(file, output)
        return output.getvalue()

    elif file_ext in ['.txt', '.md']:
        with open(filepath, 'r', encoding='utf-8') as file:
            return file.read()

    elif file_ext in ['.docx']:
        try:
            from docx import Document
            doc = Document(filepath)
            return "\n".join([para.text for para in doc.paragraphs])
        except ImportError:
            logging.error("å¤„ç†Wordæ–‡æ¡£éœ€è¦å®‰è£…python-docxåº“")
            return ""

    elif file_ext in ['.xlsx', '.xls']:
        try:
            import pandas as pd
            text = ""
            xl = pd.ExcelFile(filepath)
            for sheet_name in xl.sheet_names:
                df = xl.parse(sheet_name)
                text += f"å·¥ä½œè¡¨: {sheet_name}\n"
                text += df.to_string(index=False) + "\n\n"
            return text
        except ImportError:
            logging.error("å¤„ç†Excelæ–‡ä»¶éœ€è¦å®‰è£…pandasåº“")
            return ""

    elif file_ext in ['.pptx']:
        try:
            from pptx import Presentation
            prs = Presentation(filepath)
            text = ""
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        text += shape.text + "\n"
            return text
        except ImportError:
            logging.error("å¤„ç†PPTæ–‡ä»¶éœ€è¦å®‰è£…python-pptxåº“")
            return ""

    else:
        logging.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        return ""

# æ–°å¢ï¼šå¯ä»¥æ‰©å±•å¤šç§æ–‡ä»¶æ ¼å¼
def process_multiple_pdfs(files: List[Any], progress=gr.Progress()):
    """å¤„ç†å¤šä¸ªæ–‡ä»¶"""
    if not files:
        return "è¯·é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶(æ”¯æŒPDF, Word, Excel, PPT, TXT, Markdownç­‰)", []

    try:
        # æ¸…ç©ºå‘é‡æ•°æ®åº“å’Œç›¸å…³å­˜å‚¨
        progress(0.1, desc="æ¸…ç†å†å²æ•°æ®...")
        global faiss_index, faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index
        faiss_index = None
        faiss_contents_map = {}
        faiss_metadatas_map = {}
        faiss_id_order_for_index = []

        # æ¸…ç©ºBM25ç´¢å¼•
        BM25_MANAGER.clear()
        logging.info("æˆåŠŸæ¸…ç†å†å²FAISSæ•°æ®å’ŒBM25ç´¢å¼•")

        total_files = len(files)
        processed_results = []
        all_chunks = []
        all_metadatas = []
        all_ids = []

        for idx, file in enumerate(files, 1):
            try:
                file_name = os.path.basename(file.name)
                progress((idx - 1) / total_files, desc=f"å¤„ç†æ–‡ä»¶ {idx}/{total_files}: {file_name}")

                # æå–æ–‡æœ¬
                text = extract_text(file.name)
                if not text:
                    raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–æ–‡æœ¬")

                # åˆ†å—å¤„ç†
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=400,
                    chunk_overlap=40,
                    separators=["\n\n", "\n", "ã€‚", "ï¼Œ", "ï¼›", "ï¼š", " ", ""]
                )
                chunks = text_splitter.split_text(text)

                # ç”Ÿæˆå”¯ä¸€IDå’Œå…ƒæ•°æ®
                doc_id = f"doc_{int(time.time())}_{idx}"
                metadatas = [{"source": file_name, "doc_id": doc_id} for _ in chunks]
                chunk_ids = [f"{doc_id}_chunk_{i}" for i in range(len(chunks))]

                # ä¿å­˜æ•°æ®
                all_chunks.extend(chunks)
                all_metadatas.extend(metadatas)
                all_ids.extend(chunk_ids)

                processed_results.append(f"âœ… {file_name}: æˆåŠŸå¤„ç† {len(chunks)} ä¸ªæ–‡æœ¬å—")

            except Exception as e:
                error_msg = str(e)
                logging.error(f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {error_msg}")
                processed_results.append(f"âŒ {file_name}: å¤„ç†å¤±è´¥ - {error_msg}")

        # æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡
        if all_chunks:
            progress(0.8, desc="ç”Ÿæˆæ–‡æœ¬åµŒå…¥...")
            embeddings = EMBED_MODEL.encode(all_chunks, show_progress_bar=True)
            embeddings_np = np.array(embeddings).astype('float32')

            # æ„å»ºFAISSç´¢å¼•
            progress(0.9, desc="æ„å»ºFAISSç´¢å¼•...")
            dimension = embeddings_np.shape[1]
            faiss_index = IndexFlatL2(dimension)  # ä½¿ç”¨åŸºç¡€ç´¢å¼•

            # ç¡®ä¿å†…å®¹æ˜ å°„åŒæ­¥
            for chunk_id, chunk, meta in zip(all_ids, all_chunks, all_metadatas):
                faiss_contents_map[chunk_id] = chunk
                faiss_metadatas_map[chunk_id] = meta
                faiss_id_order_for_index.append(chunk_id)

            faiss_index.add(embeddings_np)
            logging.info(f"FAISSç´¢å¼•æ„å»ºå®Œæˆï¼Œå…±ç´¢å¼• {faiss_index.ntotal} ä¸ªæ–‡æœ¬å—")

        # æ„å»ºBM25ç´¢å¼•
        progress(0.95, desc="æ„å»ºBM25æ£€ç´¢ç´¢å¼•...")
        BM25_MANAGER.build_index(all_chunks, all_ids)

        summary = f"\næ€»è®¡å¤„ç† {total_files} ä¸ªæ–‡ä»¶ï¼Œ{len(all_chunks)} ä¸ªæ–‡æœ¬å—"
        processed_results.append(summary)

        return "\n".join(processed_results), [f"ğŸ“„ {os.path.basename(f.name)}" for f in files]

    except Exception as e:
        error_msg = str(e)
        logging.error(f"æ•´ä½“å¤„ç†è¿‡ç¨‹å‡ºé”™: {error_msg}")
        return f"å¤„ç†è¿‡ç¨‹å‡ºé”™: {error_msg}", []

# äº¤å‰ç¼–ç å™¨é‡æ’åºå‡½æ•°ï¼ˆäºŒæ¬¡æ’åºï¼‰
def rerank_with_cross_encoder(query, docs, doc_ids, metadata_list, top_k=5):
    """
    ä½¿ç”¨äº¤å‰ç¼–ç å™¨å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº

    å‚æ•°:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        docs: æ–‡æ¡£å†…å®¹åˆ—è¡¨
        doc_ids: æ–‡æ¡£IDåˆ—è¡¨
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨
        top_k: è¿”å›ç»“æœæ•°é‡

    è¿”å›:
        é‡æ’åºåçš„ç»“æœåˆ—è¡¨ [(doc_id, {'content': doc, 'metadata': metadata, 'score': score}), ...]
    """
    if not docs:
        return []

    encoder = get_cross_encoder()
    if encoder is None:
        logging.warning("äº¤å‰ç¼–ç å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡é‡æ’åº")
        # è¿”å›åŸå§‹é¡ºåºï¼ˆæŒ‰ç´¢å¼•æ’åºï¼‰
        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx / len(docs)})
                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]

    # å‡†å¤‡äº¤å‰ç¼–ç å™¨è¾“å…¥
    cross_inputs = [[query, doc] for doc in docs]

    try:
        # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
        scores = encoder.predict(cross_inputs)

        # ç»„åˆç»“æœ
        results = [
            (doc_id, {
                'content': doc,
                'metadata': meta,
                'score': float(score)  # ç¡®ä¿æ˜¯PythonåŸç”Ÿç±»å‹
            })
            for doc_id, doc, meta, score in zip(doc_ids, docs, metadata_list, scores)
        ]

        # æŒ‰å¾—åˆ†æ’åº
        results = sorted(results, key=lambda x: x[1]['score'], reverse=True)

        # è¿”å›å‰Kä¸ªç»“æœ
        return results[:top_k]
    except Exception as e:
        logging.error(f"äº¤å‰ç¼–ç å™¨é‡æ’åºå¤±è´¥: {str(e)}")
        # å‡ºé”™æ—¶è¿”å›åŸå§‹é¡ºåº
        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx / len(docs)})
                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]


# LLMç›¸å…³æ€§è¯„åˆ†å‡½æ•°
@lru_cache(maxsize=32)
def get_llm_relevance_score(query, doc):
    """
    ä½¿ç”¨LLMå¯¹æŸ¥è¯¢å’Œæ–‡æ¡£çš„ç›¸å…³æ€§è¿›è¡Œè¯„åˆ†ï¼ˆå¸¦ç¼“å­˜ï¼‰

    å‚æ•°:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        doc: æ–‡æ¡£å†…å®¹

    è¿”å›:
        ç›¸å…³æ€§å¾—åˆ† (0-10)
    """
    try:
        # æ„å»ºè¯„åˆ†æç¤ºè¯
        prompt = f"""ç»™å®šä»¥ä¸‹æŸ¥è¯¢å’Œæ–‡æ¡£ç‰‡æ®µï¼Œè¯„ä¼°å®ƒä»¬çš„ç›¸å…³æ€§ã€‚
        è¯„åˆ†æ ‡å‡†ï¼š0åˆ†è¡¨ç¤ºå®Œå…¨ä¸ç›¸å…³ï¼Œ10åˆ†è¡¨ç¤ºé«˜åº¦ç›¸å…³ã€‚
        åªéœ€è¿”å›ä¸€ä¸ª0-10ä¹‹é—´çš„æ•´æ•°åˆ†æ•°ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–è§£é‡Šã€‚

        æŸ¥è¯¢: {query}

        æ–‡æ¡£ç‰‡æ®µ: {doc}

        ç›¸å…³æ€§åˆ†æ•°(0-10):"""

        # è°ƒç”¨æœ¬åœ°LLM
        response = session.post(
            "http://localhost:11434/api/generate",
            json={
                "model": OLLAMA_MODEL_NAME,  # é€šè¿‡ç¯å¢ƒå˜é‡ OLLAMA_MODEL_NAME é…ç½®
                "prompt": prompt,
                "stream": False
            },
            timeout=180
        )

        # æå–å¾—åˆ†
        result = response.json().get("response", "").strip()

        # å°è¯•è§£æä¸ºæ•°å­—
        try:
            score = float(result)
            # ç¡®ä¿åˆ†æ•°åœ¨0-10èŒƒå›´å†…
            score = max(0, min(10, score))
            return score
        except ValueError:
            # å¦‚æœæ— æ³•è§£æä¸ºæ•°å­—ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–æ•°å­—
            match = re.search(r'\b([0-9]|10)\b', result)
            if match:
                return float(match.group(1))
            else:
                # é»˜è®¤è¿”å›ä¸­ç­‰ç›¸å…³æ€§
                return 5.0

    except Exception as e:
        logging.error(f"LLMè¯„åˆ†å¤±è´¥: {str(e)}")
        # é»˜è®¤è¿”å›ä¸­ç­‰ç›¸å…³æ€§
        return 5.0


def rerank_with_llm(query, docs, doc_ids, metadata_list, top_k=5):
    """
    ä½¿ç”¨LLMå¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº

    å‚æ•°:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        docs: æ–‡æ¡£å†…å®¹åˆ—è¡¨
        doc_ids: æ–‡æ¡£IDåˆ—è¡¨
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨
        top_k: è¿”å›ç»“æœæ•°é‡

    è¿”å›:
        é‡æ’åºåçš„ç»“æœåˆ—è¡¨
    """
    if not docs:
        return []

    results = []

    # å¯¹æ¯ä¸ªæ–‡æ¡£è¿›è¡Œè¯„åˆ†
    for doc_id, doc, meta in zip(doc_ids, docs, metadata_list):
        # è·å–LLMè¯„åˆ†
        score = get_llm_relevance_score(query, doc)

        # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
        results.append((doc_id, {
            'content': doc,
            'metadata': meta,
            'score': score / 10.0  # å½’ä¸€åŒ–åˆ°0-1
        }))

    # æŒ‰å¾—åˆ†æ’åº
    results = sorted(results, key=lambda x: x[1]['score'], reverse=True)

    # è¿”å›å‰Kä¸ªç»“æœ
    return results[:top_k]

def rerank_results(query, docs, doc_ids, metadata_list, method=None, top_k=5):
    """
    å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº

    å‚æ•°:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        docs: æ–‡æ¡£å†…å®¹åˆ—è¡¨
        doc_ids: æ–‡æ¡£IDåˆ—è¡¨
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨
        method: é‡æ’åºæ–¹æ³• ("cross_encoder", "llm" æˆ– None)
        top_k: è¿”å›ç»“æœæ•°é‡

    è¿”å›:
        é‡æ’åºåçš„ç»“æœ
    """
    # å¦‚æœæœªæŒ‡å®šæ–¹æ³•ï¼Œä½¿ç”¨å…¨å±€é…ç½®
    if method is None:
        method = RERANK_METHOD

    # æ ¹æ®æ–¹æ³•é€‰æ‹©é‡æ’åºå‡½æ•°
    if method == "llm":
        return rerank_with_llm(query, docs, doc_ids, metadata_list, top_k)
    elif method == "cross_encoder":
        return rerank_with_cross_encoder(query, docs, doc_ids, metadata_list, top_k)
    else:
        # é»˜è®¤ä¸è¿›è¡Œé‡æ’åºï¼ŒæŒ‰åŸå§‹é¡ºåºè¿”å›
        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx / len(docs)})
                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]

def stream_answer(question, enable_web_search=False, model_choice="siliconflow", progress=gr.Progress()):
    """æ”¹è¿›çš„æµå¼é—®ç­”å¤„ç†æµç¨‹ï¼Œæ”¯æŒè”ç½‘æœç´¢ã€æ··åˆæ£€ç´¢å’Œé‡æ’åºï¼Œä»¥åŠå¤šç§æ¨¡å‹é€‰æ‹©"""
    global faiss_index  # ç¡®ä¿å¯ä»¥è®¿é—®
    try:
        # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦ä¸ºç©º
        knowledge_base_exists = faiss_index is not None and faiss_index.ntotal > 0
        if not knowledge_base_exists:
            if not enable_web_search:
                yield "âš ï¸ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚", "é‡åˆ°é”™è¯¯"
                return
            else:
                logging.warning("çŸ¥è¯†åº“ä¸ºç©ºï¼Œå°†ä»…ä½¿ç”¨ç½‘ç»œæœç´¢ç»“æœ")

        progress(0.3, desc="æ‰§è¡Œé€’å½’æ£€ç´¢...")
        # ä½¿ç”¨é€’å½’æ£€ç´¢è·å–æ›´å…¨é¢çš„ç­”æ¡ˆä¸Šä¸‹æ–‡
        all_contexts, all_doc_ids, all_metadata = recursive_retrieval(
            initial_query=question,
            max_iterations=3,
            enable_web_search=enable_web_search,
            model_choice=model_choice
        )

        # ç»„åˆä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ¥æºä¿¡æ¯
        context_with_sources = []
        sources_for_conflict_detection = []

        # ä½¿ç”¨æ£€ç´¢åˆ°çš„ç»“æœæ„å»ºä¸Šä¸‹æ–‡
        for doc, doc_id, metadata in zip(all_contexts, all_doc_ids, all_metadata):
            source_type = metadata.get('source', 'æœ¬åœ°æ–‡æ¡£')

            source_item = {
                'text': doc,
                'type': source_type
            }

            if source_type == 'web':
                url = metadata.get('url', 'æœªçŸ¥URL')
                title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
                context_with_sources.append(f"[ç½‘ç»œæ¥æº: {title}] (URL: {url})\n{doc}")
                source_item['url'] = url
                source_item['title'] = title
            else:
                source = metadata.get('source', 'æœªçŸ¥æ¥æº')
                context_with_sources.append(f"[æœ¬åœ°æ–‡æ¡£: {source}]\n{doc}")
                source_item['source'] = source

            sources_for_conflict_detection.append(source_item)

        # æ£€æµ‹çŸ›ç›¾
        conflict_detected = detect_conflicts(sources_for_conflict_detection)

        # è·å–å¯ä¿¡æº
        if conflict_detected:
            credible_sources = [s for s in sources_for_conflict_detection
                                if s['type'] == 'web' and evaluate_source_credibility(s) > 0.7]

        context = "\n\n".join(context_with_sources)

        # æ·»åŠ æ—¶é—´æ•æ„Ÿæ£€æµ‹
        time_sensitive = any(word in question for word in ["æœ€æ–°", "ä»Šå¹´", "å½“å‰", "æœ€è¿‘", "åˆšåˆš"])

        # æ”¹è¿›æç¤ºè¯æ¨¡æ¿ï¼Œæé«˜å›ç­”è´¨é‡
        prompt_template = """ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ï¼Œä½ éœ€è¦åŸºäºä»¥ä¸‹{context_type}å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æä¾›çš„å‚è€ƒå†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·éµå¾ªä»¥ä¸‹å›ç­”åŸåˆ™ï¼š
1. ä»…åŸºäºæä¾›çš„å‚è€ƒå†…å®¹å›ç­”é—®é¢˜ï¼Œä¸è¦ä½¿ç”¨ä½ è‡ªå·±çš„çŸ¥è¯†
2. å¦‚æœå‚è€ƒå†…å®¹ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·å¦è¯šå‘ŠçŸ¥ä½ æ— æ³•å›ç­”
3. å›ç­”åº”è¯¥å…¨é¢ã€å‡†ç¡®ã€æœ‰æ¡ç†ï¼Œå¹¶ä½¿ç”¨é€‚å½“çš„æ®µè½å’Œç»“æ„
4. è¯·ç”¨ä¸­æ–‡å›ç­”
5. åœ¨å›ç­”æœ«å°¾æ ‡æ³¨ä¿¡æ¯æ¥æº{time_instruction}{conflict_instruction}

è¯·ç°åœ¨å¼€å§‹å›ç­”ï¼š"""

        prompt = prompt_template.format(
            context_type="æœ¬åœ°æ–‡æ¡£å’Œç½‘ç»œæœç´¢ç»“æœ" if enable_web_search and knowledge_base_exists else (
                "ç½‘ç»œæœç´¢ç»“æœ" if enable_web_search else "æœ¬åœ°æ–‡æ¡£"),
            context=context if context else (
                "ç½‘ç»œæœç´¢ç»“æœå°†ç”¨äºå›ç­”ã€‚" if enable_web_search and not knowledge_base_exists else "çŸ¥è¯†åº“ä¸ºç©ºæˆ–æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"),
            question=question,
            time_instruction="ï¼Œä¼˜å…ˆä½¿ç”¨æœ€æ–°çš„ä¿¡æ¯" if time_sensitive and enable_web_search else "",
            conflict_instruction="ï¼Œå¹¶æ˜ç¡®æŒ‡å‡ºä¸åŒæ¥æºçš„å·®å¼‚" if conflict_detected else ""
        )

        progress(0.7, desc="ç”Ÿæˆå›ç­”...")
        full_answer = ""

        # æ ¹æ®æ¨¡å‹é€‰æ‹©ä½¿ç”¨ä¸åŒçš„API
        if model_choice == "siliconflow":
            # å¯¹äºSiliconFlow APIï¼Œä¸æ”¯æŒæµå¼å“åº”ï¼Œæ‰€ä»¥ä¸€æ¬¡æ€§è·å–
            progress(0.8, desc="é€šè¿‡SiliconFlow APIç”Ÿæˆå›ç­”...")
            full_answer = call_siliconflow_api(prompt, temperature=0.7, max_tokens=1536)

            # å¤„ç†æ€ç»´é“¾
            if "<think>" in full_answer and "</think>" in full_answer:
                processed_answer = process_thinking_content(full_answer)
            else:
                processed_answer = full_answer

            yield processed_answer, "å®Œæˆ!"
        else:
            # ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹çš„æµå¼å“åº”
            response = session.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": OLLAMA_MODEL_NAME,
                    "prompt": prompt,
                    "stream": True
                },
                timeout=120,
                stream=True
            )

            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line.decode()).get("response", "")
                    full_answer += chunk

                    # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„æ€ç»´é“¾æ ‡ç­¾å¯ä»¥å¤„ç†
                    if "<think>" in full_answer and "</think>" in full_answer:
                        # éœ€è¦ç¡®ä¿å®Œæ•´æ”¶é›†ä¸€ä¸ªæ€ç»´é“¾ç‰‡æ®µåå†æ˜¾ç¤º
                        processed_answer = process_thinking_content(full_answer)
                    else:
                        processed_answer = full_answer

                    yield processed_answer, "ç”Ÿæˆå›ç­”ä¸­..."

            # å¤„ç†æœ€ç»ˆè¾“å‡ºï¼Œç¡®ä¿åº”ç”¨æ€ç»´é“¾å¤„ç†
            final_answer = process_thinking_content(full_answer)
            yield final_answer, "å®Œæˆ!"

    except Exception as e:
        yield f"ç³»ç»Ÿé”™è¯¯: {str(e)}", "é‡åˆ°é”™è¯¯"


def query_answer(question, enable_web_search=False, model_choice="siliconflow", progress=gr.Progress()):
    """é—®ç­”å¤„ç†æµç¨‹ï¼Œæ”¯æŒè”ç½‘æœç´¢ã€æ··åˆæ£€ç´¢å’Œé‡æ’åºï¼Œä»¥åŠå¤šç§æ¨¡å‹é€‰æ‹©"""
    global faiss_index  # ç¡®ä¿å¯ä»¥è®¿é—®
    try:
        logging.info(f"æ”¶åˆ°é—®é¢˜ï¼š{question}ï¼Œè”ç½‘çŠ¶æ€ï¼š{enable_web_search}ï¼Œæ¨¡å‹é€‰æ‹©ï¼š{model_choice}")

        # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦ä¸ºç©º
        knowledge_base_exists = faiss_index is not None and faiss_index.ntotal > 0
        if not knowledge_base_exists:
            if not enable_web_search:
                return "âš ï¸ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚"
            else:
                logging.warning("çŸ¥è¯†åº“ä¸ºç©ºï¼Œå°†ä»…ä½¿ç”¨ç½‘ç»œæœç´¢ç»“æœ")

        progress(0.3, desc="æ‰§è¡Œé€’å½’æ£€ç´¢...")
        # ä½¿ç”¨é€’å½’æ£€ç´¢è·å–æ›´å…¨é¢çš„ç­”æ¡ˆä¸Šä¸‹æ–‡
        all_contexts, all_doc_ids, all_metadata = recursive_retrieval(
            initial_query=question,
            max_iterations=3,
            enable_web_search=enable_web_search,
            model_choice=model_choice
        )

        # ç»„åˆä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ¥æºä¿¡æ¯
        context_with_sources = []
        sources_for_conflict_detection = []

        # ä½¿ç”¨æ£€ç´¢åˆ°çš„ç»“æœæ„å»ºä¸Šä¸‹æ–‡
        for doc, doc_id, metadata in zip(all_contexts, all_doc_ids, all_metadata):
            source_type = metadata.get('source', 'æœ¬åœ°æ–‡æ¡£')

            source_item = {
                'text': doc,
                'type': source_type
            }

            if source_type == 'web':
                url = metadata.get('url', 'æœªçŸ¥URL')
                title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
                context_with_sources.append(f"[ç½‘ç»œæ¥æº: {title}] (URL: {url})\n{doc}")
                source_item['url'] = url
                source_item['title'] = title
            else:
                source = metadata.get('source', 'æœªçŸ¥æ¥æº')
                context_with_sources.append(f"[æœ¬åœ°æ–‡æ¡£: {source}]\n{doc}")
                source_item['source'] = source

            sources_for_conflict_detection.append(source_item)

        # æ£€æµ‹çŸ›ç›¾
        conflict_detected = detect_conflicts(sources_for_conflict_detection)

        # è·å–å¯ä¿¡æº
        if conflict_detected:
            credible_sources = [s for s in sources_for_conflict_detection
                                if s['type'] == 'web' and evaluate_source_credibility(s) > 0.7]

        context = "\n\n".join(context_with_sources)

        # æ·»åŠ æ—¶é—´æ•æ„Ÿæ£€æµ‹
        time_sensitive = any(word in question for word in ["æœ€æ–°", "ä»Šå¹´", "å½“å‰", "æœ€è¿‘", "åˆšåˆš"])

        # æ”¹è¿›æç¤ºè¯æ¨¡æ¿ï¼Œæé«˜å›ç­”è´¨é‡
        prompt_template = """ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ï¼Œä½ éœ€è¦åŸºäºä»¥ä¸‹{context_type}å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æä¾›çš„å‚è€ƒå†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·éµå¾ªä»¥ä¸‹å›ç­”åŸåˆ™ï¼š
1. ä»…åŸºäºæä¾›çš„å‚è€ƒå†…å®¹å›ç­”é—®é¢˜ï¼Œä¸è¦ä½¿ç”¨ä½ è‡ªå·±çš„çŸ¥è¯†
2. å¦‚æœå‚è€ƒå†…å®¹ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·å¦è¯šå‘ŠçŸ¥ä½ æ— æ³•å›ç­”
3. å›ç­”åº”è¯¥å…¨é¢ã€å‡†ç¡®ã€æœ‰æ¡ç†ï¼Œå¹¶ä½¿ç”¨é€‚å½“çš„æ®µè½å’Œç»“æ„
4. è¯·ç”¨ä¸­æ–‡å›ç­”
5. åœ¨å›ç­”æœ«å°¾æ ‡æ³¨ä¿¡æ¯æ¥æº{time_instruction}{conflict_instruction}

è¯·ç°åœ¨å¼€å§‹å›ç­”ï¼š"""

        prompt = prompt_template.format(
            context_type="æœ¬åœ°æ–‡æ¡£å’Œç½‘ç»œæœç´¢ç»“æœ" if enable_web_search and knowledge_base_exists else (
                "ç½‘ç»œæœç´¢ç»“æœ" if enable_web_search else "æœ¬åœ°æ–‡æ¡£"),
            context=context if context else (
                "ç½‘ç»œæœç´¢ç»“æœå°†ç”¨äºå›ç­”ã€‚" if enable_web_search and not knowledge_base_exists else "çŸ¥è¯†åº“ä¸ºç©ºæˆ–æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"),
            question=question,
            time_instruction="ï¼Œä¼˜å…ˆä½¿ç”¨æœ€æ–°çš„ä¿¡æ¯" if time_sensitive and enable_web_search else "",
            conflict_instruction="ï¼Œå¹¶æ˜ç¡®æŒ‡å‡ºä¸åŒæ¥æºçš„å·®å¼‚" if conflict_detected else ""
        )

        progress(0.8, desc="ç”Ÿæˆå›ç­”...")

        # æ ¹æ®æ¨¡å‹é€‰æ‹©ä½¿ç”¨ä¸åŒçš„API
        if model_choice == "siliconflow":
            # ä½¿ç”¨SiliconFlow API
            result = call_siliconflow_api(prompt, temperature=0.7, max_tokens=1536)

            # å¤„ç†æ€ç»´é“¾
            processed_result = process_thinking_content(result)
            return processed_result
        else:
            # ä½¿ç”¨æœ¬åœ°Ollama
            response = session.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": OLLAMA_MODEL_NAME,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=180,  # å»¶é•¿åˆ°3åˆ†é’Ÿ
                headers={'Connection': 'close'}  # æ·»åŠ è¿æ¥å¤´
            )
            response.raise_for_status()  # æ£€æŸ¥HTTPçŠ¶æ€ç 

            progress(1.0, desc="å®Œæˆ!")
            # ç¡®ä¿è¿”å›å­—ç¬¦ä¸²å¹¶å¤„ç†ç©ºå€¼
            result = response.json()
            return process_thinking_content(str(result.get("response", "æœªè·å–åˆ°æœ‰æ•ˆå›ç­”")))

    except json.JSONDecodeError:
        return "å“åº”è§£æå¤±è´¥ï¼Œè¯·é‡è¯•"
    except KeyError:
        return "å“åº”æ ¼å¼å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æœåŠ¡"
    except Exception as e:
        progress(1.0, desc="é‡åˆ°é”™è¯¯")  # ç¡®ä¿è¿›åº¦æ¡å®Œæˆ
        return f"ç³»ç»Ÿé”™è¯¯: {str(e)}"


def process_thinking_content(text):
    """å¤„ç†åŒ…å«<think>æ ‡ç­¾çš„å†…å®¹ï¼Œå°†å…¶è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
    # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºæœ‰æ•ˆæ–‡æœ¬
    if text is None:
        return ""

    # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
    if not isinstance(text, str):
        try:
            processed_text = str(text)
        except:
            return "æ— æ³•å¤„ç†çš„å†…å®¹æ ¼å¼"
    else:
        processed_text = text

    # å¤„ç†æ€ç»´é“¾æ ‡ç­¾
    try:
        while "<think>" in processed_text and "</think>" in processed_text:
            start_idx = processed_text.find("<think>")
            end_idx = processed_text.find("</think>")
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                thinking_content = processed_text[start_idx + 7:end_idx]
                before_think = processed_text[:start_idx]
                after_think = processed_text[end_idx + 8:]

                # ä½¿ç”¨å¯æŠ˜å è¯¦æƒ…æ¡†æ˜¾ç¤ºæ€ç»´é“¾
                processed_text = before_think + "\n\n<details>\n<summary>æ€è€ƒè¿‡ç¨‹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>\n\n" + thinking_content + "\n\n</details>\n\n" + after_think

        # å¤„ç†å…¶ä»–HTMLæ ‡ç­¾ï¼Œä½†ä¿ç•™detailså’Œsummaryæ ‡ç­¾
        processed_html = []
        i = 0
        while i < len(processed_text):
            if processed_text[i:i + 8] == "<details" or processed_text[i:i + 9] == "</details" or \
                    processed_text[i:i + 8] == "<summary" or processed_text[i:i + 9] == "</summary":
                # ä¿ç•™è¿™äº›æ ‡ç­¾
                tag_end = processed_text.find(">", i)
                if tag_end != -1:
                    processed_html.append(processed_text[i:tag_end + 1])
                    i = tag_end + 1
                    continue

            if processed_text[i] == "<":
                processed_html.append("&lt;")
            elif processed_text[i] == ">":
                processed_html.append("&gt;")
            else:
                processed_html.append(processed_text[i])
            i += 1

        processed_text = "".join(processed_html)
    except Exception as e:
        logging.error(f"å¤„ç†æ€ç»´é“¾å†…å®¹æ—¶å‡ºé”™: {str(e)}")
        # å‡ºé”™æ—¶è‡³å°‘è¿”å›åŸå§‹æ–‡æœ¬ï¼Œä½†ç¡®ä¿å®‰å…¨å¤„ç†HTMLæ ‡ç­¾
        try:
            return text.replace("<", "&lt;").replace(">", "&gt;")
        except:
            return "å¤„ç†å†…å®¹æ—¶å‡ºé”™"

    return processed_text


def call_siliconflow_api(prompt, temperature=0.7, max_tokens=1024):
    """
    è°ƒç”¨SiliconFlow APIè·å–å›ç­”

    Args:
        prompt: æç¤ºè¯
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°

    Returns:
        ç”Ÿæˆçš„å›ç­”æ–‡æœ¬å’Œæ€ç»´é“¾å†…å®¹
    """
    # æ£€æŸ¥æ˜¯å¦é…ç½®äº†SiliconFlow APIå¯†é’¥
    if not SILICONFLOW_API_KEY:
        logging.error("æœªè®¾ç½® SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡ã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„ API å¯†é’¥ã€‚")
        return "é”™è¯¯ï¼šæœªé…ç½® SiliconFlow API å¯†é’¥ã€‚", ""

    try:
        payload = {
            "model": SILICONFLOW_MODEL_NAME,  # é€šè¿‡ç¯å¢ƒå˜é‡ SILICONFLOW_MODEL_NAME é…ç½®
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "stream": False,
            "max_tokens": max_tokens,
            "stop": None,
            "temperature": temperature,
            "top_p": 0.7,
            "top_k": 50,
            "frequency_penalty": 0.5,
            "n": 1,
            "response_format": {"type": "text"}
        }

        headers = {
            "Authorization": f"Bearer {SILICONFLOW_API_KEY.strip()}",  # ä»ç¯å¢ƒå˜é‡è·å–å¯†é’¥å¹¶å»é™¤ç©ºæ ¼
            "Content-Type": "application/json; charset=utf-8"  # æ˜ç¡®æŒ‡å®šç¼–ç 
        }

        # æ‰‹åŠ¨å°†payloadç¼–ç ä¸ºUTF-8 JSONå­—ç¬¦ä¸²
        json_payload = json.dumps(payload, ensure_ascii=False).encode('utf-8')

        response = requests.post(
            SILICONFLOW_API_URL,
            data=json_payload,  # é€šè¿‡dataå‚æ•°å‘é€ç¼–ç åçš„JSON
            headers=headers,
            timeout=180  # å»¶é•¿è¶…æ—¶æ—¶é—´åˆ°3åˆ†é’Ÿ
        )

        response.raise_for_status()
        result = response.json()

        # æå–å›ç­”å†…å®¹å’Œæ€ç»´é“¾
        if "choices" in result and len(result["choices"]) > 0:
            message = result["choices"][0]["message"]
            content = message.get("content", "")
            reasoning = message.get("reasoning_content", "")

            # å¦‚æœæœ‰æ€ç»´é“¾ï¼Œåˆ™æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œä»¥ä¾¿å‰ç«¯å¤„ç†
            if reasoning:
                # æ·»åŠ æ€ç»´é“¾æ ‡è®°
                full_response = f"{content}<think>{reasoning}</think>"
                return full_response
            else:
                return content
        else:
            return "APIè¿”å›ç»“æœæ ¼å¼å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥"

    except requests.exceptions.RequestException as e:
        logging.error(f"è°ƒç”¨SiliconFlow APIæ—¶å‡ºé”™: {str(e)}")
        return f"è°ƒç”¨APIæ—¶å‡ºé”™: {str(e)}"
    except json.JSONDecodeError:
        logging.error("SiliconFlow APIè¿”å›éJSONå“åº”")
        return "APIå“åº”è§£æå¤±è´¥"
    except Exception as e:
        logging.error(f"è°ƒç”¨SiliconFlow APIæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        return f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}"

# åˆå¹¶è¯­ä¹‰æœç´¢å’ŒBM25æœç´¢ç»“æœ
def hybrid_merge(semantic_results, bm25_results, alpha=0.7):
    """
    åˆå¹¶è¯­ä¹‰æœç´¢å’ŒBM25æœç´¢ç»“æœ

    å‚æ•°:
        semantic_results: å‘é‡æ£€ç´¢ç»“æœ (å­—å…¸æ ¼å¼ï¼ŒåŒ…å«ids, documents, metadatas)
        bm25_results: BM25æ£€ç´¢ç»“æœ (å­—å…¸åˆ—è¡¨ï¼ŒåŒ…å«id, score, content)
        alpha: è¯­ä¹‰æœç´¢æƒé‡ (0-1)

    è¿”å›:
        åˆå¹¶åçš„ç»“æœåˆ—è¡¨ [(doc_id, {'score': score, 'content': content, 'metadata': metadata}), ...]
    """
    merged_dict = {}
    global faiss_metadatas_map  # Ensure we can access the global map

    # å¤„ç†è¯­ä¹‰æœç´¢ç»“æœ
    if (semantic_results and
            isinstance(semantic_results.get('documents'), list) and len(semantic_results['documents']) > 0 and
            isinstance(semantic_results.get('metadatas'), list) and len(semantic_results['metadatas']) > 0 and
            isinstance(semantic_results.get('ids'), list) and len(semantic_results['ids']) > 0 and
            isinstance(semantic_results['documents'][0], list) and
            isinstance(semantic_results['metadatas'][0], list) and
            isinstance(semantic_results['ids'][0], list) and
            len(semantic_results['documents'][0]) == len(semantic_results['metadatas'][0]) == len(
                semantic_results['ids'][0])):

        num_results = len(semantic_results['documents'][0])
        # Assuming semantic_results are already ordered by relevance (higher is better)
        # A simple rank-based score, can be replaced if actual scores/distances are available and preferred
        for i, (doc_id, doc, meta) in enumerate(
                zip(semantic_results['ids'][0], semantic_results['documents'][0], semantic_results['metadatas'][0])):
            score = 1.0 - (i / max(1, num_results))  # Higher rank (smaller i) gets higher score
            merged_dict[doc_id] = {
                'score': alpha * score,
                'content': doc,
                'metadata': meta
            }
    else:
        logging.warning(
            "Semantic results are missing, have an unexpected format, or are empty. Skipping semantic part in hybrid merge.")

    # å¤„ç†BM25ç»“æœ
    if not bm25_results:
        return sorted(merged_dict.items(), key=lambda x: x[1]['score'], reverse=True)

    valid_bm25_scores = [r['score'] for r in bm25_results if isinstance(r, dict) and 'score' in r]
    max_bm25_score = max(valid_bm25_scores) if valid_bm25_scores else 1.0

    for result in bm25_results:
        if not (isinstance(result, dict) and 'id' in result and 'score' in result and 'content' in result):
            logging.warning(f"Skipping invalid BM25 result item: {result}")
            continue

        doc_id = result['id']
        # Normalize BM25 score
        normalized_score = result['score'] / max_bm25_score if max_bm25_score > 0 else 0

        if doc_id in merged_dict:
            merged_dict[doc_id]['score'] += (1 - alpha) * normalized_score
        else:
            metadata = faiss_metadatas_map.get(doc_id, {})  # Get metadata from our global map
            merged_dict[doc_id] = {
                'score': (1 - alpha) * normalized_score,
                'content': result['content'],
                'metadata': metadata
            }

    merged_results = sorted(merged_dict.items(), key=lambda x: x[1]['score'], reverse=True)
    return merged_results

def update_bm25_index():
    """æ›´æ–°BM25ç´¢å¼•ï¼Œä»å†…å­˜ä¸­çš„æ˜ å°„åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
    global faiss_contents_map, faiss_id_order_for_index
    try:
        # Use the ordered list of IDs to ensure consistency
        doc_ids = faiss_id_order_for_index
        if not doc_ids:
            logging.warning("æ²¡æœ‰å¯ç´¢å¼•çš„æ–‡æ¡£ (FAISS IDåˆ—è¡¨ä¸ºç©º)")
            BM25_MANAGER.clear()
            return False

        # Retrieve documents in the correct order
        documents = [faiss_contents_map.get(doc_id, "") for doc_id in doc_ids]

        # Filter out any potential empty documents if necessary, though map access should be safe
        valid_docs_with_ids = [(doc_id, doc) for doc_id, doc in zip(doc_ids, documents) if doc]
        if not valid_docs_with_ids:
            logging.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£å†…å®¹å¯ç”¨äºBM25ç´¢å¼•")
            BM25_MANAGER.clear()
            return False

        # Separate IDs and documents again for building the index
        final_doc_ids = [item[0] for item in valid_docs_with_ids]
        final_documents = [item[1] for item in valid_docs_with_ids]

        BM25_MANAGER.build_index(final_documents, final_doc_ids)
        logging.info(f"BM25ç´¢å¼•æ›´æ–°å®Œæˆï¼Œå…±ç´¢å¼• {len(final_doc_ids)} ä¸ªæ–‡æ¡£")
        return True
    except Exception as e:
        logging.error(f"æ›´æ–°BM25ç´¢å¼•å¤±è´¥: {str(e)}")
        return False

def get_system_models_info():
    """è¿”å›ç³»ç»Ÿä½¿ç”¨çš„å„ç§æ¨¡å‹ä¿¡æ¯"""
    models_info = {
        "åµŒå…¥æ¨¡å‹": "all-MiniLM-L6-v2",
        "åˆ†å—æ–¹æ³•": "RecursiveCharacterTextSplitter (chunk_size=800, overlap=150)",
        "æ£€ç´¢æ–¹æ³•": "å‘é‡æ£€ç´¢ + BM25æ··åˆæ£€ç´¢ (Î±=0.7)",
        "é‡æ’åºæ¨¡å‹": "äº¤å‰ç¼–ç å™¨ (sentence-transformers/distiluse-base-multilingual-cased-v2)",
        "ç”Ÿæˆæ¨¡å‹(Ollama)": OLLAMA_MODEL_NAME,
        "ç”Ÿæˆæ¨¡å‹(SiliconFlow)": SILICONFLOW_MODEL_NAME,
        "åˆ†è¯å·¥å…·": "jieba (ä¸­æ–‡åˆ†è¯)"
    }
    return models_info


# ä¿®æ”¹å…¨å±€ç¼“å­˜å˜é‡ä¸ºå­—å…¸æ ¼å¼ï¼Œä¾¿äºé€šè¿‡IDå¿«é€ŸæŸ¥æ‰¾
chunk_data_cache = {}  # æ ¼å¼: {chunk_id: chunk_data}
def get_document_chunks(progress=gr.Progress()):
    """è·å–æ–‡æ¡£åˆ†å—ç»“æœç”¨äºå¯è§†åŒ–"""
    global faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index
    global chunk_data_cache  # å£°æ˜ä½¿ç”¨å…¨å±€ç¼“å­˜

    try:
        progress(0.1, desc="æ­£åœ¨ä»å†…å­˜åŠ è½½æ•°æ®...")

        # æ¸…ç©ºæ—§ç¼“å­˜
        chunk_data_cache.clear()

        if not faiss_id_order_for_index:
            return [], "çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£ï¼Œè¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£ã€‚"

        progress(0.3, desc="æ­£åœ¨ç»„ç»‡åˆ†å—æ•°æ®...")

        # æŒ‰åŸå§‹å¤„ç†é¡ºåºç»„ç»‡æ•°æ®
        table_data = []
        for idx, chunk_id in enumerate(faiss_id_order_for_index):
            content = faiss_contents_map.get(chunk_id, "")
            meta = faiss_metadatas_map.get(chunk_id, {})

            if not content:
                continue

            # æ„å»ºåˆ†å—æ•°æ®å¯¹è±¡
            chunk_data = {
                "row_id": idx,  # è¡¨æ ¼è¡Œå·
                "chunk_id": chunk_id,
                "source": meta.get("source", "æœªçŸ¥æ¥æº"),
                "content": content,
                "preview": content[:200] + "..." if len(content) > 200 else content,
                "char_count": len(content),
                "token_count": len(list(jieba.cut(content)))
            }

            # æ·»åŠ åˆ°ç¼“å­˜å’Œè¡¨æ ¼æ•°æ®
            chunk_data_cache[idx] = chunk_data  # ç”¨è¡Œå·ä½œä¸ºé”®
            table_data.append([
                chunk_data["source"],
                f"{idx + 1}/{len(faiss_id_order_for_index)}",
                chunk_data["char_count"],
                chunk_data["token_count"],
                chunk_data["preview"]
            ])

        progress(1.0, desc="æ•°æ®åŠ è½½å®Œæˆ!")
        return table_data, f"å…± {len(table_data)} ä¸ªæ–‡æœ¬å—"

    except Exception as e:
        chunk_data_cache.clear()
        return [], f"è·å–åˆ†å—æ•°æ®å¤±è´¥: {str(e)}"


def show_chunk_details(evt: gr.SelectData):
    """æ˜¾ç¤ºé€‰ä¸­åˆ†å—çš„è¯¦ç»†å†…å®¹"""
    try:
        if not evt.index or evt.index[0] is None:
            return "æœªé€‰æ‹©æœ‰æ•ˆè¡Œ"

        row_idx = evt.index[0]  # è·å–è¡Œç´¢å¼•

        # ä»ç¼“å­˜è·å–æ•°æ®
        selected_chunk = chunk_data_cache.get(row_idx)
        if not selected_chunk:
            return "æœªæ‰¾åˆ°å¯¹åº”çš„åˆ†å—æ•°æ®"

        # æ ¼å¼åŒ–æ˜¾ç¤ºè¯¦æƒ…
        detail = f"""
        [æ¥æº] {selected_chunk['source']}
        [ID] {selected_chunk['chunk_id']}
        [å­—ç¬¦æ•°] {selected_chunk['char_count']}
        [åˆ†è¯æ•°] {selected_chunk['token_count']}
        ----------------------------
        {selected_chunk['content']}
        """
        return detail

    except Exception as e:
        return f"åŠ è½½åˆ†å—è¯¦æƒ…å¤±è´¥: {str(e)}"

# ä¿®æ”¹å¸ƒå±€éƒ¨åˆ†ï¼Œæ·»åŠ ä¸€ä¸ªæ–°çš„æ ‡ç­¾é¡µ
# ä¿®æ”¹å¸ƒå±€éƒ¨åˆ†ï¼Œæ·»åŠ çœŸå®ç³»ç»Ÿç›‘æ§åŠŸèƒ½
with gr.Blocks(
        title="æœ¬åœ°RAGé—®ç­”ç³»ç»Ÿ",
        css="""
    /* å…¨å±€ä¸»é¢˜å˜é‡ */
    :root[data-theme="light"] {
        --text-color: #2c3e50;
        --bg-color: #ffffff;
        --panel-bg: #f8f9fa;
        --border-color: #e9ecef;
        --success-color: #4CAF50;
        --error-color: #f44336;
        --primary-color: #2196F3;
        --secondary-bg: #ffffff;
        --hover-color: #e9ecef;
        --chat-user-bg: #e3f2fd;
        --chat-assistant-bg: #f5f5f5;
        --tech-blue: #0d47a1;
        --tech-purple: #7b1fa2;
        --tech-cyan: #00bcd4;
    }

    :root[data-theme="dark"] {
        --text-color: #e0e0e0;
        --bg-color: #1a1a1a;
        --panel-bg: #2d2d2d;
        --border-color: #404040;
        --success-color: #81c784;
        --error-color: #e57373;
        --primary-color: #64b5f6;
        --secondary-bg: #2d2d2d;
        --hover-color: #404040;
        --chat-user-bg: #1e3a5f;
        --chat-assistant-bg: #2d2d2d;
        --tech-blue: #1e88e5;
        --tech-purple: #9c27b0;
        --tech-cyan: #00e5ff;
    }

    /* å…¨å±€æ ·å¼ */
    body {
        font-family: 'Roboto', 'Segoe UI', sans-serif;
        margin: 0;
        padding: 0;
        overflow-x: hidden;
        width: 100vw;
        height: 100vh;
        background: linear-gradient(135deg, var(--bg-color) 0%, #1a1a2e 100%);
    }

    .gradio-container {
        max-width: 100% !important;
        width: 100% !important;
        margin: 0 !important;
        padding: 0 1% !important;
        color: var(--text-color);
        background-color: transparent;
        min-height: 100vh;
    }

    /* ç¡®ä¿æ ‡ç­¾å†…å®¹æ’‘æ»¡ */
    .tabs.svelte-710i53 {
        margin: 0 !important;
        padding: 0 !important;
        width: 100% !important;
    }

    /* ä¸»é¢˜åˆ‡æ¢æŒ‰é’® */
    .theme-toggle {
        position: fixed;
        top: 20px;
        right: 20px;
        z-index: 1000;
        padding: 8px 16px;
        border-radius: 20px;
        border: 1px solid var(--border-color);
        background: var(--panel-bg);
        color: var(--text-color);
        cursor: pointer;
        transition: all 0.3s ease;
        font-size: 14px;
        display: flex;
        align-items: center;
        gap: 8px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }

    .theme-toggle:hover {
        background: var(--hover-color);
        transform: translateY(-2px);
        box-shadow: 0 6px 8px rgba(0,0,0,0.15);
    }

    /* é¢æ¿æ ·å¼ */
    .left-panel {
        padding-right: 20px;
        border-right: 1px solid var(--border-color);
        background: rgba(30, 30, 46, 0.7);
        width: 100%;
        border-radius: 12px;
        backdrop-filter: blur(10px);
        box-shadow: 0 8px 32px rgba(0,0,0,0.2);
    }

    .right-panel {
        height: 100vh;
        background: rgba(30, 30, 46, 0.7);
        width: 100%;
        border-radius: 12px;
        backdrop-filter: blur(10px);
        box-shadow: 0 8px 32px rgba(0,0,0,0.2);
    }

    /* æ–‡ä»¶åˆ—è¡¨æ ·å¼ */
    .file-list {
        margin-top: 10px;
        padding: 12px;
        background: rgba(45, 45, 70, 0.6);
        border-radius: 8px;
        font-size: 14px;
        line-height: 1.6;
        border: 1px solid rgba(100, 100, 150, 0.3);
    }

    /* ç­”æ¡ˆæ¡†æ ·å¼ */
    .answer-box {
        min-height: 500px !important;
        background: rgba(45, 45, 70, 0.6);
        border-radius: 8px;
        padding: 16px;
        font-size: 15px;
        line-height: 1.6;
        border: 1px solid rgba(100, 100, 150, 0.3);
    }

    /* è¾“å…¥æ¡†æ ·å¼ */
    textarea {
        background: rgba(45, 45, 70, 0.6) !important;
        color: var(--text-color) !important;
        border: 1px solid rgba(100, 100, 150, 0.3) !important;
        border-radius: 8px !important;
        padding: 12px !important;
        font-size: 14px !important;
        transition: all 0.3s ease;
    }

    textarea:focus {
        border-color: var(--tech-cyan) !important;
        box-shadow: 0 0 0 2px rgba(0, 188, 212, 0.2);
    }

    /* æŒ‰é’®æ ·å¼ */
    button.primary {
        background: linear-gradient(135deg, var(--tech-blue) 0%, var(--tech-purple) 100%) !important;
        color: white !important;
        border-radius: 8px !important;
        padding: 10px 20px !important;
        font-weight: 500 !important;
        transition: all 0.3s ease !important;
        border: none;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }

    button.primary:hover {
        opacity: 0.9;
        transform: translateY(-2px);
        box-shadow: 0 6px 8px rgba(0,0,0,0.15);
    }

    /* æ ‡é¢˜å’Œæ–‡æœ¬æ ·å¼ */
    h1, h2, h3 {
        color: var(--text-color) !important;
        font-weight: 600 !important;
    }

    .footer-note {
        color: var(--text-color);
        opacity: 0.8;
        font-size: 13px;
        margin-top: 12px;
    }

    /* åŠ è½½å’Œè¿›åº¦æ ·å¼ */
    #loading, .progress-text {
        color: var(--text-color);
    }

    /* èŠå¤©è®°å½•æ ·å¼ */
    .chat-container {
        border: 1px solid rgba(100, 100, 150, 0.3);
        border-radius: 8px;
        margin-bottom: 16px;
        max-height: 80vh;
        height: 80vh !important;
        overflow-y: auto;
        background: rgba(45, 45, 70, 0.6);
    }

    .chat-message {
        padding: 12px 16px;
        margin: 8px;
        border-radius: 8px;
        font-size: 14px;
        line-height: 1.5;
        position: relative;
        overflow: hidden;
    }

    .chat-message.user {
        background: linear-gradient(135deg, rgba(30, 58, 95, 0.8) 0%, rgba(30, 30, 70, 0.8) 100%);
        margin-left: 32px;
        border-top-right-radius: 4px;
        border-left: 3px solid var(--tech-cyan);
    }

    .chat-message.assistant {
        background: linear-gradient(135deg, rgba(45, 45, 70, 0.8) 0%, rgba(30, 30, 50, 0.8) 100%);
        margin-right: 32px;
        border-top-left-radius: 4px;
        border-right: 3px solid var(--tech-purple);
    }

    .chat-message .timestamp {
        font-size: 12px;
        color: var(--text-color);
        opacity: 0.7;
        margin-bottom: 4px;
    }

    .chat-message .content {
        white-space: pre-wrap;
    }

    /* æŒ‰é’®ç»„æ ·å¼ */
    .button-row {
        display: flex;
        gap: 8px;
        margin-top: 8px;
    }

    .clear-button {
        background: linear-gradient(135deg, #f44336 0%, #c62828 100%) !important;
    }

    /* APIé…ç½®æç¤ºæ ·å¼ */
    .api-info {
        margin-top: 10px;
        padding: 10px;
        border-radius: 5px;
        background: rgba(45, 45, 70, 0.6);
        border: 1px solid rgba(100, 100, 150, 0.3);
    }

    /* æ–°å¢: æ•°æ®å¯è§†åŒ–å¡ç‰‡æ ·å¼ */
    .model-card {
        background: rgba(45, 45, 70, 0.6);
        border-radius: 8px;
        padding: 16px;
        border: 1px solid rgba(100, 100, 150, 0.3);
        margin-bottom: 16px;
    }

    .model-card h3 {
        margin-top: 0;
        border-bottom: 1px solid rgba(100, 100, 150, 0.3);
        padding-bottom: 8px;
        color: var(--tech-cyan);
    }

    .model-item {
        display: flex;
        margin-bottom: 8px;
    }

    .model-item .label {
        flex: 1;
        font-weight: 500;
        color: var(--tech-cyan);
    }

    .model-item .value {
        flex: 2;
    }

    /* æ•°æ®è¡¨æ ¼æ ·å¼ */
    .chunk-table {
        border-radius: 8px;
        overflow: hidden;
        border: 1px solid rgba(100, 100, 150, 0.3);
        background: rgba(45, 45, 70, 0.6);
    }

    .chunk-table th, .chunk-table td {
        border: 1px solid rgba(100, 100, 150, 0.3);
        padding: 8px;
    }

    .chunk-detail-box {
        min-height: 200px;
        padding: 16px;
        background: rgba(45, 45, 70, 0.6);
        border-radius: 8px;
        border: 1px solid rgba(100, 100, 150, 0.3);
        font-family: monospace;
        white-space: pre-wrap;
        overflow-y: auto;
    }

    /* æ–°å¢: ç³»ç»Ÿç›‘æ§é¢æ¿æ ·å¼ */
    .monitor-panel {
        background: rgba(30, 30, 46, 0.7);
        border-radius: 12px;
        padding: 20px;
        margin-bottom: 20px;
        backdrop-filter: blur(10px);
        box-shadow: 0 8px 32px rgba(0,0,0,0.2);
        border: 1px solid rgba(100, 100, 150, 0.3);
    }

    .monitor-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        margin-bottom: 15px;
        padding-bottom: 10px;
        border-bottom: 1px solid rgba(100, 100, 150, 0.3);
    }

    .monitor-title {
        font-size: 18px;
        font-weight: 600;
        color: var(--tech-cyan);
    }

    .monitor-refresh {
        background: transparent;
        border: none;
        color: var(--tech-cyan);
        cursor: pointer;
        font-size: 14px;
        display: flex;
        align-items: center;
        gap: 5px;
    }

    .monitor-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
        gap: 20px;
    }

    .metric-card {
        background: rgba(45, 45, 70, 0.6);
        border-radius: 10px;
        padding: 15px;
        border: 1px solid rgba(100, 100, 150, 0.3);
    }

    .metric-title {
        font-size: 14px;
        margin-bottom: 10px;
        color: var(--tech-cyan);
    }

    .metric-value {
        font-size: 24px;
        font-weight: 700;
        margin-bottom: 5px;
    }

    .metric-trend {
        font-size: 12px;
        color: #4CAF50;
    }

    .metric-trend.negative {
        color: #f44336;
    }

    .metric-chart {
        height: 100px;
        margin-top: 10px;
        position: relative;
    }

    .chart-bar {
        position: absolute;
        bottom: 0;
        width: 8px;
        background: var(--tech-cyan);
        border-radius: 4px 4px 0 0;
        transition: height 0.5s ease;
    }

    .log-container {
        max-height: 300px;
        overflow-y: auto;
        background: rgba(20, 20, 35, 0.8);
        border-radius: 8px;
        padding: 15px;
        font-family: monospace;
        font-size: 13px;
        line-height: 1.5;
    }

    .log-entry {
        margin-bottom: 8px;
        padding-bottom: 8px;
        border-bottom: 1px solid rgba(100, 100, 150, 0.2);
    }

    .log-time {
        color: var(--tech-cyan);
        margin-right: 10px;
    }

    .log-info {
        color: #4CAF50;
    }

    .log-warning {
        color: #FFC107;
    }

    .log-error {
        color: #f44336;
    }

    /* æ–°å¢: ç§‘æŠ€æ„Ÿè£…é¥°å…ƒç´  */
    .tech-grid {
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        pointer-events: none;
        z-index: -1;
        opacity: 0.05;
    }

    .grid-line {
        position: absolute;
        background: var(--tech-cyan);
    }

    .grid-horizontal {
        width: 100%;
        height: 1px;
        top: 0;
        left: 0;
    }

    .grid-vertical {
        height: 100%;
        width: 1px;
        top: 0;
        left: 0;
    }

    /* æ–°å¢: éœ“è™¹æ•ˆæœ */
    .neon-text {
        text-shadow: 0 0 5px var(--tech-cyan), 0 0 10px var(--tech-cyan), 0 0 15px var(--tech-purple);
    }

    /* æ–°å¢: è¿›åº¦æ¡æ ·å¼ */
    .progress-container {
        width: 100%;
        background: rgba(255,255,255,0.1);
        border-radius: 10px;
        margin: 10px 0;
    }

    .progress-bar {
        height: 8px;
        border-radius: 10px;
        background: linear-gradient(90deg, var(--tech-cyan), var(--tech-purple));
        transition: width 0.3s ease;
    }
    """
) as demo:
    # æ·»åŠ ç§‘æŠ€æ„Ÿç½‘æ ¼èƒŒæ™¯
    gr.HTML("""
    <div class="tech-grid">
        <div class="grid-line grid-horizontal" style="top: 0;"></div>
        <div class="grid-line grid-horizontal" style="top: 20%;"></div>
        <div class="grid-line grid-horizontal" style="top: 40%;"></div>
        <div class="grid-line grid-horizontal" style="top: 60%;"></div>
        <div class="grid-line grid-horizontal" style="top: 80%;"></div>
        <div class="grid-line grid-horizontal" style="top: 100%;"></div>

        <div class="grid-line grid-vertical" style="left: 0;"></div>
        <div class="grid-line grid-vertical" style="left: 20%;"></div>
        <div class="grid-line grid-vertical" style="left: 40%;"></div>
        <div class="grid-line grid-vertical" style="left: 60%;"></div>
        <div class="grid-line grid-vertical" style="left: 80%;"></div>
        <div class="grid-line grid-vertical" style="left: 100%;"></div>
    </div>
    """)

    gr.Markdown("# ğŸ§  <span class='neon-text'>æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ</span>")

    with gr.Tabs() as tabs:
        # ç¬¬ä¸€ä¸ªé€‰é¡¹å¡ï¼šé—®ç­”å¯¹è¯
        with gr.TabItem("ğŸ’¬ é—®ç­”å¯¹è¯"):
            with gr.Row(equal_height=True):
                # å·¦ä¾§æ“ä½œé¢æ¿ - è°ƒæ•´æ¯”ä¾‹ä¸ºåˆé€‚çš„å¤§å°
                with gr.Column(scale=5, elem_classes="left-panel"):
                    gr.Markdown("## ğŸ“‚ æ–‡æ¡£å¤„ç†åŒº")
                    with gr.Group():
                        # åœ¨ Gradio UI éƒ¨åˆ†ä¿®æ”¹æ–‡ä»¶ä¸Šä¼ ç»„ä»¶
                        file_input = gr.File(
                            label="ä¸Šä¼ æ–‡æ¡£ (æ”¯æŒPDF, Word, Excel, PPT, TXT, Markdownç­‰)",
                            file_types=[".pdf", ".txt", ".docx", ".xlsx", ".xls", ".pptx", ".md"],
                            file_count="multiple"
                        )
                        upload_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")
                        upload_status = gr.Textbox(
                            label="å¤„ç†çŠ¶æ€",
                            interactive=False,
                            lines=2
                        )
                        file_list = gr.Textbox(
                            label="å·²å¤„ç†æ–‡ä»¶",
                            interactive=False,
                            lines=3,
                            elem_classes="file-list"
                        )

                    # å°†é—®é¢˜è¾“å…¥åŒºç§»è‡³å·¦ä¾§é¢æ¿åº•éƒ¨
                    gr.Markdown("## â“ è¾“å…¥é—®é¢˜")
                    with gr.Group():
                        question_input = gr.Textbox(
                            label="è¾“å…¥é—®é¢˜",
                            lines=3,
                            placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                            elem_id="question-input"
                        )
                        with gr.Row():
                            # æ·»åŠ è”ç½‘å¼€å…³
                            web_search_checkbox = gr.Checkbox(
                                label="å¯ç”¨è”ç½‘æœç´¢",
                                value=False,
                                info="æ‰“å¼€åå°†åŒæ—¶æœç´¢ç½‘ç»œå†…å®¹ï¼ˆéœ€é…ç½®SERPAPI_KEYï¼‰"
                            )

                            # æ·»åŠ æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
                            model_choice = gr.Dropdown(
                                choices=["ollama", "siliconflow"],
                                value="ollama",
                                label="æ¨¡å‹é€‰æ‹©",
                                info="é€‰æ‹©ä½¿ç”¨æœ¬åœ°æ¨¡å‹æˆ–äº‘ç«¯æ¨¡å‹"
                            )

                        with gr.Row():
                            ask_btn = gr.Button("ğŸ” å¼€å§‹æé—®", variant="primary", scale=2)
                            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary", elem_classes="clear-button",
                                                  scale=1)

                    # æ·»åŠ APIé…ç½®æç¤ºä¿¡æ¯
                    api_info = gr.HTML(
                        """
                        <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
                            <p>ğŸ“¢ <strong>åŠŸèƒ½è¯´æ˜ï¼š</strong></p>
                            <p>1. <strong>è”ç½‘æœç´¢</strong>ï¼š%s</p>
                            <p>2. <strong>æ¨¡å‹é€‰æ‹©</strong>ï¼šå½“å‰ä½¿ç”¨ <strong>%s</strong> %s</p>
                        </div>
                        """
                    )

                # å³ä¾§å¯¹è¯åŒº - è°ƒæ•´æ¯”ä¾‹
                with gr.Column(scale=7, elem_classes="right-panel"):
                    gr.Markdown("## ğŸ“ å¯¹è¯è®°å½•")

                    # å¯¹è¯è®°å½•æ˜¾ç¤ºåŒº
                    chatbot = gr.Chatbot(
                        label="å¯¹è¯å†å²",
                        height=600,  # å¢åŠ é«˜åº¦
                        elem_classes="chat-container",
                        show_label=False
                    )

                    status_display = gr.HTML("", elem_id="status-display")
                    gr.Markdown("""
                    <div class="footer-note">
                        *å›ç­”ç”Ÿæˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…<br>
                        *æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯åŸºäºå‰æ–‡ç»§ç»­æé—®
                    </div>
                    """)

        # ç¬¬äºŒä¸ªé€‰é¡¹å¡ï¼šåˆ†å—å¯è§†åŒ–
        with gr.TabItem("ğŸ“Š åˆ†å—å¯è§†åŒ–"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("## ğŸ’¡ ç³»ç»Ÿæ¨¡å‹ä¿¡æ¯")

                    # æ˜¾ç¤ºç³»ç»Ÿæ¨¡å‹ä¿¡æ¯å¡ç‰‡
                    models_info = get_system_models_info()
                    with gr.Group(elem_classes="model-card"):
                        gr.Markdown("### æ ¸å¿ƒæ¨¡å‹ä¸æŠ€æœ¯")

                        for key, value in models_info.items():
                            with gr.Row():
                                gr.Markdown(f"**{key}**:", elem_classes="label")
                                gr.Markdown(f"{value}", elem_classes="value")

                with gr.Column(scale=2):
                    gr.Markdown("## ğŸ“„ æ–‡æ¡£åˆ†å—ç»Ÿè®¡")
                    refresh_chunks_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ†å—æ•°æ®", variant="primary")
                    chunks_status = gr.Markdown("ç‚¹å‡»æŒ‰é’®æŸ¥çœ‹åˆ†å—ç»Ÿè®¡")

            # åˆ†å—æ•°æ®è¡¨æ ¼å’Œè¯¦æƒ…
            with gr.Row():
                chunks_data = gr.Dataframe(
                    headers=["æ¥æº", "åºå·", "å­—ç¬¦æ•°", "åˆ†è¯æ•°", "å†…å®¹é¢„è§ˆ"],
                    elem_classes="chunk-table",
                    interactive=False,
                    wrap=True,
                    row_count=(10, "dynamic")
                )

            with gr.Row():
                chunk_detail_text = gr.Textbox(
                    label="åˆ†å—è¯¦æƒ…",
                    placeholder="ç‚¹å‡»è¡¨æ ¼ä¸­çš„è¡ŒæŸ¥çœ‹å®Œæ•´å†…å®¹...",
                    lines=8,
                    elem_classes="chunk-detail-box"
                )

            gr.Markdown("""
            <div class="footer-note">
                * ç‚¹å‡»è¡¨æ ¼ä¸­çš„è¡Œå¯æŸ¥çœ‹è¯¥åˆ†å—çš„å®Œæ•´å†…å®¹<br>
                * åˆ†è¯æ•°è¡¨ç¤ºä½¿ç”¨jiebaåˆ†è¯åçš„tokenæ•°é‡
            </div>
            """)

        # æ–°å¢ç¬¬ä¸‰ä¸ªé€‰é¡¹å¡ï¼šç³»ç»Ÿç›‘æ§
        with gr.TabItem("ğŸ“ˆ ç³»ç»Ÿç›‘æ§"):
            with gr.Column():
                # ç³»ç»Ÿèµ„æºç›‘æ§é¢æ¿
                with gr.Group(elem_classes="monitor-panel"):
                    with gr.Row():
                        gr.Markdown("## ğŸ–¥ï¸ ç³»ç»Ÿèµ„æºç›‘æ§", elem_classes="monitor-title")
                        refresh_monitor_btn = gr.Button("ğŸ”„ åˆ·æ–°æ•°æ®", variant="primary", elem_classes="monitor-refresh")

                    with gr.Row(elem_classes="monitor-grid"):
                        # CPUä½¿ç”¨ç‡
                        with gr.Column():
                            cpu_card = gr.Group(elem_classes="metric-card")
                            with cpu_card:
                                gr.Markdown("CPUä½¿ç”¨ç‡", elem_classes="metric-title")
                                cpu_value = gr.Markdown("åŠ è½½ä¸­...", elem_classes="metric-value")
                                cpu_progress = gr.HTML("""
                                    <div class="progress-container">
                                        <div class="progress-bar" style="width: 0%"></div>
                                    </div>
                                """)
                                cpu_info = gr.Markdown("æ ¸å¿ƒæ•°: åŠ è½½ä¸­...", elem_classes="metric-trend")

                        # å†…å­˜ä½¿ç”¨
                        with gr.Column():
                            memory_card = gr.Group(elem_classes="metric-card")
                            with memory_card:
                                gr.Markdown("å†…å­˜ä½¿ç”¨", elem_classes="metric-title")
                                memory_value = gr.Markdown("åŠ è½½ä¸­...", elem_classes="metric-value")
                                memory_progress = gr.HTML("""
                                    <div class="progress-container">
                                        <div class="progress-bar" style="width: 0%"></div>
                                    </div>
                                """)
                                memory_info = gr.Markdown("æ€»å†…å­˜: åŠ è½½ä¸­...", elem_classes="metric-trend")

                        # ç£ç›˜ç©ºé—´
                        with gr.Column():
                            disk_card = gr.Group(elem_classes="metric-card")
                            with disk_card:
                                gr.Markdown("ç£ç›˜ç©ºé—´", elem_classes="metric-title")
                                disk_value = gr.Markdown("åŠ è½½ä¸­...", elem_classes="metric-value")
                                disk_progress = gr.HTML("""
                                    <div class="progress-container">
                                        <div class="progress-bar" style="width: 0%"></div>
                                    </div>
                                """)
                                disk_info = gr.Markdown("æ€»ç©ºé—´: åŠ è½½ä¸­...", elem_classes="metric-trend")

                        # ç½‘ç»œæµé‡
                        with gr.Column():
                            network_card = gr.Group(elem_classes="metric-card")
                            with network_card:
                                gr.Markdown("ç½‘ç»œæµé‡", elem_classes="metric-title")
                                network_value = gr.Markdown("åŠ è½½ä¸­...", elem_classes="metric-value")
                                network_info = gr.Markdown("ä¸Šä¼ /ä¸‹è½½: 0 KB/s", elem_classes="metric-trend")

                # æ€§èƒ½æŒ‡æ ‡é¢æ¿
                with gr.Group(elem_classes="monitor-panel"):
                    gr.Markdown("## âš¡ æ€§èƒ½æŒ‡æ ‡", elem_classes="monitor-title")

                    with gr.Row(elem_classes="monitor-grid"):
                        # å“åº”æ—¶é—´
                        with gr.Column():
                            latency_card = gr.Group(elem_classes="metric-card")
                            with latency_card:
                                gr.Markdown("å¹³å‡å“åº”æ—¶é—´", elem_classes="metric-title")
                                latency_value = gr.Markdown("0 ms", elem_classes="metric-value")
                                latency_info = gr.Markdown("å†å²è®°å½•: åŠ è½½ä¸­...", elem_classes="metric-trend")

                        # è¯·æ±‚é€Ÿç‡
                        with gr.Column():
                            request_card = gr.Group(elem_classes="metric-card")
                            with request_card:
                                gr.Markdown("è¯·æ±‚ç»Ÿè®¡", elem_classes="metric-title")
                                request_value = gr.Markdown("æ€»è¯·æ±‚: 0", elem_classes="metric-value")
                                request_info = gr.Markdown("æˆåŠŸ/å¤±è´¥: 0/0", elem_classes="metric-trend")

                        # å‘é‡æ•°æ®åº“
                        with gr.Column():
                            vector_db_card = gr.Group(elem_classes="metric-card")
                            with vector_db_card:
                                gr.Markdown("å‘é‡æ•°æ®åº“", elem_classes="metric-title")
                                vector_db_value = gr.Markdown("åˆ†å—æ•°: 0", elem_classes="metric-value")
                                vector_db_info = gr.Markdown("å‘é‡æ•°: 0", elem_classes="metric-trend")

                        # æ¨¡å‹çŠ¶æ€
                        with gr.Column():
                            model_status_card = gr.Group(elem_classes="metric-card")
                            with model_status_card:
                                gr.Markdown("æ¨¡å‹çŠ¶æ€", elem_classes="metric-title")
                                model_status_value = gr.Markdown("çŠ¶æ€: æœªçŸ¥", elem_classes="metric-value")
                                model_status_info = gr.Markdown("è¿æ¥: æ£€æŸ¥ä¸­...", elem_classes="metric-trend")

                # ç³»ç»Ÿæ—¥å¿—é¢æ¿
                with gr.Group(elem_classes="monitor-panel"):
                    gr.Markdown("## ğŸ“ ç³»ç»Ÿæ—¥å¿—", elem_classes="monitor-title")

                    # æ—¥å¿—ç­›é€‰é€‰é¡¹
                    with gr.Row():
                        log_level = gr.Dropdown(
                            choices=["æ‰€æœ‰çº§åˆ«", "ä¿¡æ¯", "è­¦å‘Š", "é”™è¯¯"],
                            value="æ‰€æœ‰çº§åˆ«",
                            label="æ—¥å¿—çº§åˆ«"
                        )
                        log_search = gr.Textbox(
                            label="æœç´¢æ—¥å¿—",
                            placeholder="è¾“å…¥å…³é”®è¯æœç´¢..."
                        )
                        clear_logs_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºæ—¥å¿—", variant="secondary")

                    # æ—¥å¿—æ˜¾ç¤ºåŒºåŸŸ
                    log_display = gr.HTML("", elem_classes="log-container")

    # è¿›åº¦æ˜¾ç¤ºç»„ä»¶è°ƒæ•´åˆ°å·¦ä¾§é¢æ¿ä¸‹æ–¹
    with gr.Row(visible=False) as progress_row:
        gr.HTML("""
        <div class="progress-text">
            <span>å½“å‰è¿›åº¦ï¼š</span>
            <span id="current-step" style="color: #2b6de3;">åˆå§‹åŒ–...</span>
            <span id="progress-percent" style="margin-left:15px;color: #e32b2b;">0%</span>
        </div>
        """)


    # å®šä¹‰å‡½æ•°å¤„ç†äº‹ä»¶
    def clear_chat_history():
        return None, "å¯¹è¯å·²æ¸…ç©º"


    def process_chat(question: str, history: Optional[List[Tuple[str, str]]], enable_web_search: bool,
                     model_choice: str):
        if history is None:
            history = []

        # æ›´æ–°æ¨¡å‹é€‰æ‹©ä¿¡æ¯çš„æ˜¾ç¤º
        api_text = """
        <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
            <p>ğŸ“¢ <strong>åŠŸèƒ½è¯´æ˜ï¼š</strong></p>
            <p>1. <strong>è”ç½‘æœç´¢</strong>ï¼š%s</p>
            <p>2. <strong>æ¨¡å‹é€‰æ‹©</strong>ï¼šå½“å‰ä½¿ç”¨ <strong>%s</strong> %s</p>
        </div>
        """ % (
            "å·²å¯ç”¨" if enable_web_search else "æœªå¯ç”¨",
            "Cloud DeepSeek-R1 æ¨¡å‹" if model_choice == "siliconflow" else "æœ¬åœ° Ollama æ¨¡å‹",
            "(éœ€è¦åœ¨.envæ–‡ä»¶ä¸­é…ç½®SERPAPI_KEY)" if enable_web_search else ""
        )

        # å¦‚æœé—®é¢˜ä¸ºç©ºï¼Œä¸å¤„ç†
        if not question or question.strip() == "":
            history.append(("", "é—®é¢˜ä¸èƒ½ä¸ºç©ºï¼Œè¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜ã€‚"))
            return history, "", api_text

        # æ·»åŠ ç”¨æˆ·é—®é¢˜åˆ°å†å²
        history.append((question, ""))

        # åˆ›å»ºç”Ÿæˆå™¨
        resp_generator = stream_answer(question, enable_web_search, model_choice)

        # æµå¼æ›´æ–°å›ç­”
        for response, status in resp_generator:
            history[-1] = (question, response)
            yield history, "", api_text


    def update_api_info(enable_web_search, model_choice):
        api_text = """
        <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
            <p>ğŸ“¢ <strong>åŠŸèƒ½è¯´æ˜ï¼š</strong></p>
            <p>1. <strong>è”ç½‘æœç´¢</strong>ï¼š%s</p>
            <p>2. <strong>æ¨¡å‹é€‰æ‹©</strong>ï¼šå½“å‰ä½¿ç”¨ <strong>%s</strong> %s</p>
        </div>
        """ % (
            "å·²å¯ç”¨" if enable_web_search else "æœªå¯ç”¨",
            "Cloud DeepSeek-R1 æ¨¡å‹" if model_choice == "siliconflow" else "æœ¬åœ° Ollama æ¨¡å‹",
            "(éœ€è¦åœ¨.envæ–‡ä»¶ä¸­é…ç½®SERPAPI_KEY)" if enable_web_search else ""
        )
        return api_text


    # æ–°å¢ï¼šçœŸå®ç³»ç»Ÿç›‘æ§æ•°æ®è·å–å‡½æ•°
    def get_real_system_metrics():
        """è·å–çœŸå®çš„ç³»ç»Ÿç›‘æ§æ•°æ®"""
        try:
            import psutil
            from datetime import datetime

            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            cpu_count = psutil.cpu_count(logical=False)  # ç‰©ç†æ ¸å¿ƒæ•°

            # å†…å­˜ä½¿ç”¨
            mem = psutil.virtual_memory()
            memory_total = round(mem.total / (1024 ** 3), 1)  # GB
            memory_used = round(mem.used / (1024 ** 3), 1)
            memory_percent = mem.percent

            # ç£ç›˜ä½¿ç”¨
            disk = psutil.disk_usage('/')
            disk_total = round(disk.total / (1024 ** 3), 1)
            disk_used = round(disk.used / (1024 ** 3), 1)
            disk_percent = disk.percent

            # ç½‘ç»œæµé‡
            net_io = psutil.net_io_counters()
            network_up = round(net_io.bytes_sent / 1024, 1)  # KB
            network_down = round(net_io.bytes_recv / 1024, 1)

            # å‘é‡æ•°æ®åº“çœŸå®ä¿¡æ¯
            doc_count = len(faiss_contents_map) if faiss_contents_map else 0
            vector_count = faiss_index.ntotal if faiss_index else 0

            # æ¨¡å‹è¿æ¥çŠ¶æ€
            model_status = "ç¦»çº¿"
            model_connection = "æœªè¿æ¥"

            current_model_choice = "siliconflow"  # é»˜è®¤å€¼

            # ä¿®å¤ï¼šæ ¹æ®å½“å‰å®é™…ä½¿ç”¨çš„æ¨¡å‹æ¥æ£€æµ‹çŠ¶æ€
            def check_model_status(model_choice):
                try:
                    # ç›´æ¥æµ‹è¯•å½“å‰å®é™…åœ¨ä½¿ç”¨çš„API
                    if model_choice == "siliconflow":
                        # æ›´å¥å£®çš„SiliconFlow APIæµ‹è¯•
                        test_response = call_siliconflow_api("æµ‹è¯•è¿æ¥", max_tokens=5)
                        if test_response and isinstance(test_response, str):
                            return "åœ¨çº¿", "SiliconFlow APIæ­£å¸¸"
                        return "åœ¨çº¿", "APIå“åº”æ­£å¸¸"
                    else:
                        # åªæœ‰å½“é€‰æ‹©ollamaæ—¶æ‰æ£€æµ‹æœ¬åœ°æœåŠ¡
                        try:
                            response = requests.get(
                                "http://localhost:11434/api/tags",
                                timeout=10,
                                headers={'Connection': 'close'}
                            )
                            if response.status_code == 200:
                                return "åœ¨çº¿", "OllamaæœåŠ¡æ­£å¸¸"
                            return "åœ¨çº¿", f"HTTP {response.status_code}"
                        except requests.exceptions.ConnectionError:
                            return "ç¦»çº¿", "OllamaæœåŠ¡æœªå¯åŠ¨"
                        except Exception as e:
                            return "åœ¨çº¿", f"è¿æ¥å¼‚å¸¸: {str(e)}"
                except Exception as e:
                    return "ç¦»çº¿", f"æ£€æµ‹å¤±è´¥: {str(e)}"

            model_status, model_connection = check_model_status(current_model_choice)

            # ç”Ÿæˆè¿›åº¦æ¡HTML
            def create_progress_bar(percent, color="var(--tech-cyan)"):
                return f"""
                <div class="progress-container">
                    <div class="progress-bar" style="width: {percent}%; background: {color}"></div>
                </div>
                """

            cpu_color = "#4CAF50" if cpu_percent < 50 else "#FFC107" if cpu_percent < 80 else "#f44336"
            cpu_progress = create_progress_bar(cpu_percent, cpu_color)

            mem_color = "#4CAF50" if memory_percent < 50 else "#FFC107" if memory_percent < 80 else "#f44336"
            memory_progress = create_progress_bar(memory_percent, mem_color)

            disk_color = "#4CAF50" if disk_percent < 50 else "#FFC107" if disk_percent < 80 else "#f44336"
            disk_progress = create_progress_bar(disk_percent, disk_color)

            # ç”Ÿæˆæ—¥å¿—
            log_entries = []
            current_time = datetime.now().strftime("%H:%M:%S")

            # ç³»ç»Ÿæ—¥å¿—
            log_entries.append(f"""
            <div class="log-entry">
                <span class="log-time">[{current_time}]</span>
                <span class="log-info">[INFO]</span> ç³»ç»Ÿç›‘æ§æ•°æ®å·²æ›´æ–°
            </div>
            """)

            # è­¦å‘Šæ—¥å¿—
            if cpu_percent > 80:
                log_entries.append(f"""
                <div class="log-entry">
                    <span class="log-time">[{current_time}]</span>
                    <span class="log-warning">[WARNING]</span> CPUä½¿ç”¨ç‡è¿‡é«˜: {cpu_percent}%
                </div>
                """)

            if memory_percent > 80:
                log_entries.append(f"""
                <div class="log-entry">
                    <span class="log-time">[{current_time}]</span>
                    <span class="log-warning">[WARNING]</span> å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_percent}%
                </div>
                """)

            if disk_percent > 90:
                log_entries.append(f"""
                <div class="log-entry">
                    <span class="log-time">[{current_time}]</span>
                    <span class="log-error">[ERROR]</span> ç£ç›˜ç©ºé—´ä¸è¶³: {disk_percent}%
                </div>
                """)

            log_html = "".join(log_entries[-10:])  # åªæ˜¾ç¤ºæœ€è¿‘10æ¡æ—¥å¿—

            return (
                f"{cpu_percent}%",  # cpu_value
                cpu_progress,  # cpu_progress
                f"ç‰©ç†æ ¸å¿ƒ: {cpu_count}",  # cpu_info
                f"{memory_used}GB / {memory_total}GB",  # memory_value
                memory_progress,  # memory_progress
                f"ä½¿ç”¨ç‡: {memory_percent}%",  # memory_info
                f"{disk_used}GB / {disk_total}GB",  # disk_value
                disk_progress,  # disk_progress
                f"ä½¿ç”¨ç‡: {disk_percent}%",  # disk_info
                f"â†‘ {network_up}KB â†“ {network_down}KB",  # network_value
                f"ç´¯è®¡æµé‡",  # network_info
                f"{int(time.time() - psutil.boot_time())}s",  # latency_value (ç³»ç»Ÿè¿è¡Œæ—¶é—´)
                f"ç³»ç»Ÿè¿è¡Œæ—¶é—´",  # latency_info
                f"{doc_count + vector_count}",  # request_value
                f"æ–‡æ¡£: {doc_count} | å‘é‡: {vector_count}",  # request_info
                f"åˆ†å—æ•°: {doc_count}",  # vector_db_value
                f"å‘é‡æ•°: {vector_count}",  # vector_db_info
                f"çŠ¶æ€: {model_status}",  # model_status_value
                f"è¿æ¥: {model_connection}",  # model_status_info
                log_html  # log_display
            )

        except Exception as e:
            error_msg = f"ç›‘æ§æ•°æ®è·å–å¤±è´¥: {str(e)}"
            return (
                "é”™è¯¯", "", error_msg,
                "é”™è¯¯", "", error_msg,
                "é”™è¯¯", "", error_msg,
                "é”™è¯¯", error_msg,
                "é”™è¯¯", error_msg,
                "é”™è¯¯", error_msg,
                "é”™è¯¯", error_msg,
                "é”™è¯¯", error_msg,
                f"<div class='log-error'>[ERROR] {error_msg}</div>"
            )


    # æ–°å¢ï¼šæ¸…ç©ºæ—¥å¿—å‡½æ•°
    def clear_system_logs():
        return "<div class='log-info'>æ—¥å¿—å·²æ¸…ç©º</div>"


    # ç»‘å®šUIäº‹ä»¶
    upload_btn.click(
        process_multiple_pdfs,
        inputs=[file_input],
        outputs=[upload_status, file_list],
        show_progress=True
    )

    # ç»‘å®šæé—®æŒ‰é’®
    ask_btn.click(
        process_chat,
        inputs=[question_input, chatbot, web_search_checkbox, model_choice],
        outputs=[chatbot, question_input, api_info]
    )

    # ç»‘å®šæ¸…ç©ºæŒ‰é’®
    clear_btn.click(
        clear_chat_history,
        inputs=[],
        outputs=[chatbot, status_display]
    )

    # å½“åˆ‡æ¢è”ç½‘æœç´¢æˆ–æ¨¡å‹é€‰æ‹©æ—¶æ›´æ–°APIä¿¡æ¯
    web_search_checkbox.change(
        update_api_info,
        inputs=[web_search_checkbox, model_choice],
        outputs=[api_info]
    )

    model_choice.change(
        update_api_info,
        inputs=[web_search_checkbox, model_choice],
        outputs=[api_info]
    )

    # æ–°å¢ï¼šåˆ†å—å¯è§†åŒ–åˆ·æ–°æŒ‰é’®äº‹ä»¶
    refresh_chunks_btn.click(
        fn=get_document_chunks,
        outputs=[chunks_data, chunks_status]
    )

    # æ–°å¢ï¼šåˆ†å—è¡¨æ ¼ç‚¹å‡»äº‹ä»¶
    chunks_data.select(
        fn=show_chunk_details,
        outputs=chunk_detail_text
    )

    # æ–°å¢ï¼šç³»ç»Ÿç›‘æ§åˆ·æ–°æŒ‰é’®äº‹ä»¶ - ä½¿ç”¨çœŸå®æ•°æ®
    refresh_monitor_btn.click(
        fn=get_real_system_metrics,
        outputs=[
            cpu_value, cpu_progress, cpu_info,
            memory_value, memory_progress, memory_info,
            disk_value, disk_progress, disk_info,
            network_value, network_info,
            latency_value, latency_info,
            request_value, request_info,
            vector_db_value, vector_db_info,
            model_status_value, model_status_info,
            log_display
        ]
    )

    # æ–°å¢ï¼šæ¸…ç©ºæ—¥å¿—æŒ‰é’®äº‹ä»¶
    clear_logs_btn.click(
        fn=clear_system_logs,
        outputs=[log_display]
    )

# ä¿®æ”¹JavaScriptæ³¨å…¥éƒ¨åˆ†
demo._js = """
function gradioApp() {
    // è®¾ç½®é»˜è®¤ä¸»é¢˜ä¸ºæš—è‰²
    document.documentElement.setAttribute('data-theme', 'dark');

    const observer = new MutationObserver((mutations) => {
        document.getElementById("loading").style.display = "none";
        const progress = document.querySelector('.progress-text');
        if (progress) {
            const percent = document.querySelector('.progress > div')?.innerText || '';
            const step = document.querySelector('.progress-description')?.innerText || '';
            document.getElementById('current-step').innerText = step;
            document.getElementById('progress-percent').innerText = percent;
        }
    });
    observer.observe(document.body, {childList: true, subtree: true});
}

function toggleTheme() {
    const root = document.documentElement;
    const currentTheme = root.getAttribute('data-theme');
    const newTheme = currentTheme === 'light' ? 'dark' : 'light';
    root.setAttribute('data-theme', newTheme);
}

// åˆå§‹åŒ–ä¸»é¢˜å’Œè‡ªåŠ¨åˆ·æ–°
document.addEventListener('DOMContentLoaded', () => {
    document.documentElement.setAttribute('data-theme', 'dark');

    // æ·»åŠ åŠ¨ç”»æ•ˆæœ
    setTimeout(() => {
        const elements = document.querySelectorAll('.chat-message, .metric-card, .model-card');
        elements.forEach(el => {
            el.style.opacity = '0';
            el.style.transform = 'translateY(20px)';
            el.style.transition = 'opacity 0.5s ease, transform 0.5s ease';
        });

        setTimeout(() => {
            elements.forEach((el, index) => {
                setTimeout(() => {
                    el.style.opacity = '1';
                    el.style.transform = 'translateY(0)';
                }, index * 100);
            });
        }, 300);
    }, 500);

    // ç³»ç»Ÿç›‘æ§é¡µé¢è‡ªåŠ¨åˆ·æ–°
    let refreshInterval;
    const monitorTab = document.querySelector('[data-testid="tab-ğŸ“ˆ ç³»ç»Ÿç›‘æ§"]');
    if (monitorTab) {
        monitorTab.addEventListener('click', () => {
            // æ¸…é™¤ç°æœ‰å®šæ—¶å™¨
            if (refreshInterval) clearInterval(refreshInterval);

            // æ¯10ç§’è‡ªåŠ¨åˆ·æ–°ç›‘æ§æ•°æ®
            refreshInterval = setInterval(() => {
                const refreshBtn = document.querySelector('button[value="ğŸ”„ åˆ·æ–°æ•°æ®"]');
                if (refreshBtn) refreshBtn.click();
            }, 10000);
        });
    }
});
"""


# æ–°å¢ï¼šå‘é‡æ•°æ®åº“ä¿¡æ¯è·å–å‡½æ•°ï¼ˆéœ€è¦åœ¨vector_storeæ¨¡å—ä¸­å®ç°ï¼‰
def get_vector_store_info():
    """è·å–å‘é‡æ•°æ®åº“çš„çœŸå®ç»Ÿè®¡ä¿¡æ¯"""
    try:
        if faiss_index is None:
            return {'document_count': 0, 'chunk_count': 0}

        # æ–‡æ¡£æ•° = å”¯ä¸€æ–‡æ¡£IDæ•°é‡ï¼ˆæ ¹æ®å…ƒæ•°æ®ä¸­çš„doc_idï¼‰
        doc_ids = set()
        chunk_count = 0

        # éå†æ‰€æœ‰å…ƒæ•°æ®è®°å½•
        for meta in faiss_metadatas_map.values():
            if 'doc_id' in meta:
                doc_ids.add(meta['doc_id'])
            chunk_count += 1

        # å¦‚æœå…ƒæ•°æ®ä¸­æ²¡æœ‰doc_idï¼Œåˆ™ä½¿ç”¨åŸå§‹IDè®¡æ•°
        if not doc_ids:
            doc_ids = set(faiss_contents_map.keys())

        return {
            'document_count': len(doc_ids),
            'chunk_count': faiss_index.ntotal if faiss_index else chunk_count
        }
    except Exception as e:
        logging.error(f"è·å–å‘é‡æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {str(e)}")
        return {'document_count': 0, 'chunk_count': 0}


# ä¿®æ”¹ç«¯å£æ£€æŸ¥å‡½æ•°
def is_port_available(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        return s.connect_ex(('127.0.0.1', port)) != 0  # æ›´å¯é çš„æ£€æµ‹æ–¹å¼


def check_environment():
    """ç¯å¢ƒä¾èµ–æ£€æŸ¥ï¼ˆäº‘ç«¯APIç‰ˆæœ¬ï¼‰"""
    # æ£€æŸ¥ SiliconFlow API å¯†é’¥
    if not SILICONFLOW_API_KEY:
        print("âŒ æœªé…ç½® SiliconFlow API å¯†é’¥")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® SILICONFLOW_API_KEY")
        return False

    print("âœ… SiliconFlow API å¯†é’¥å·²é…ç½®")
    print("âœ… è·³è¿‡æœ¬åœ° Ollama æ£€æŸ¥ï¼Œä½¿ç”¨äº‘ç«¯ API æ¨¡å¼")

    # æµ‹è¯• SiliconFlow API è¿æ¥
    try:
        test_prompt = "ä½ å¥½ï¼Œè¯·å›å¤'è¿æ¥æˆåŠŸ'"
        result = call_siliconflow_api(test_prompt, temperature=0.1, max_tokens=50)
        if "è¿æ¥æˆåŠŸ" in result or "ä½ å¥½" in result:
            print("âœ… SiliconFlow API è¿æ¥æµ‹è¯•æˆåŠŸ")
            return True
        else:
            print("âš ï¸ SiliconFlow API å“åº”å¼‚å¸¸ï¼Œä½†ç»§ç»­è¿è¡Œ")
            return True
    except Exception as e:
        print(f"âš ï¸ SiliconFlow API æµ‹è¯•å¤±è´¥: {e}")
        print("âš ï¸ ç»§ç»­è¿è¡Œï¼Œè¯·ç¡®ä¿ API å¯†é’¥æ­£ç¡®")
        return True


# æ–¹æ¡ˆ2ï¼šç¦ç”¨æµè§ˆå™¨ç¼“å­˜ï¼ˆæ·»åŠ metaæ ‡ç­¾ï¼‰
gr.HTML("""
<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
<meta http-equiv="Pragma" content="no-cache">
<meta http-equiv="Expires" content="0">
""")

if __name__ == "__main__":
    # æ£€æŸ¥è¿œç¨‹ API å’Œæ¨¡å‹
    if not check_environment():
        exit(1)

    # æœ¬åœ°ç«¯å£é€‰æ‹©é€»è¾‘
    ports = [17995, 17996, 17997, 17998, 17999]
    selected_port = next((p for p in ports if is_port_available(p)), None)

    if not selected_port:
        print("æ‰€æœ‰ç«¯å£éƒ½è¢«å ç”¨ï¼Œè¯·æ‰‹åŠ¨é‡Šæ”¾ç«¯å£")
        exit(1)

    try:
        # æ‰“å¼€æµè§ˆå™¨
        webbrowser.open(f"http://127.0.0.1:{selected_port}")

        # å¯åŠ¨ Demo
        demo.launch(
            server_port=selected_port,
            server_name="0.0.0.0",
            show_error=True,
            ssl_verify=False,
            height=900
        )
    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥: {str(e)}")

